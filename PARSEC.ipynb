{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.A.R.S.E.C. - PDF Analysis and Review System for Exam Content\n",
    "\n",
    "This Jupyter Notebook aims to provide a system for analyzing and reviewing PDF documents related to exam content. The notebook will include various functionalities such as parsing PDF files, extracting text and metadata, performing text analysis, generating visualizations, and facilitating the review process. The goal is to create an efficient and comprehensive system for working with exam-related PDF documents.\n",
    "\n",
    "## Requirements\n",
    "- python 3.10.11\n",
    "- Elastic Cloud instance w/ ELSER Model\n",
    "- nvidia gpu or llm api key\n",
    "\n",
    "# Optional\n",
    "- Langsmith Account/API key (free tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m pip install -qU elasticsearch langchain langchain-elasticsearch openai tiktoken PyPDF4\n",
    "\n",
    "#%pip install PyPDF4\n",
    "#%pip install elasticsearch\n",
    "#%pip install langchain\n",
    "#%pip install langchain-elasticsearch\n",
    "#%pip install openai tiktoken\n",
    "#%pip install wordcloud\n",
    "#%pip install rapidocr-onnxruntime\n",
    "#%pip install nltk\n",
    "#%pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import importlib\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Union, Any\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "#from concurrent.futures import ProcessPoolExecutor\n",
    "#from tqdm import tqdm\n",
    "#import jq\n",
    "#from langchain_community.document_loaders import JSONLoader # for non-windows users I suppose\n",
    "\n",
    "\n",
    "# Download the Punkt tokenizer models (only needed once)\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the PDF Files\n",
    "Get rid of pesky passwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decrypted_SEC595 - Book 1_2036060.pdf', 'decrypted_SEC595 - Book 2_2036060.pdf', 'decrypted_SEC595 - Book 3_2036060.pdf', 'decrypted_SEC595 - Book 4_2036060.pdf', 'decrypted_SEC595 - Book 5_2036060.pdf', 'decrypted_SEC595 - Book 6_2036060.pdf', 'decrypted_SEC595 - Workbook 1_2036060.pdf', 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'SEC595 - Book 1_2036060.pdf', 'SEC595 - Book 2_2036060.pdf', 'SEC595 - Book 3_2036060.pdf', 'SEC595 - Book 4_2036060.pdf', 'SEC595 - Book 5_2036060.pdf', 'SEC595 - Book 6_2036060.pdf', 'SEC595 - Workbook 1_2036060.pdf', 'SEC595 - Workbook 2_2036060.pdf']\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "pdf_files = [f for f in os.listdir(cwd) if f.endswith('.pdf')]\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already decrypted\n"
     ]
    }
   ],
   "source": [
    "# Skip if already run before\n",
    "if 'decrypted_'+pdf_files[-1] in os.listdir(cwd):\n",
    "    print('Already decrypted')\n",
    "else:\n",
    "    # Decrypt the PDF files\n",
    "    for i, pdf_file in enumerate(pdf_files):\n",
    "        if pdf_file.startswith('decrypted_'):\n",
    "            continue\n",
    "        with open(pdf_file, 'rb') as file:\n",
    "            print(pdf_file, ' is decrypting')\n",
    "            reader = PdfReader(file)\n",
    "            # Attempt to decrypt the PDF with an empty password\n",
    "            if reader.is_encrypted:\n",
    "                try:\n",
    "                    reader.decrypt('PpH[uQ(7+Gy:FdA9;X9QVXi@$zVwD-') # Your password here\n",
    "                except:\n",
    "                    print(\"The PDF is encrypted and cannot be decrypted with the password.\")\n",
    "                    break\n",
    "            \n",
    "            writer = PdfWriter()\n",
    "            \n",
    "            # Copy the content from the original PDF to the new PDF\n",
    "            for page_num in range(len(reader.pages)):\n",
    "                page = reader.pages[page_num]\n",
    "                writer.add_page(page)\n",
    "            \n",
    "            # Save the new PDF file without encryption\n",
    "            with open('decrypted_'+pdf_files[i], 'wb') as output_file:\n",
    "                writer.write(output_file)\n",
    "            print(\"The PDF has been successfully decrypted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text and metadata\n",
    "We'll load the decrypted pdfs, split pages, and so on.\n",
    "There are books and workbooks within the provided materials and they both require some cleaning. Basically everything after the SANS copyright is useless. For each book we'll take two slices of pages, [4:-1] & [-1]. The former is the course content and the latter is the provided index. For workbooks, we only need one slice [4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "pdfs = [f for f in os.listdir(cwd) if f.startswith('decrypted_')]\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "indexes = []\n",
    "contents = []\n",
    "for pdf in pdfs:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    pages = loader.load_and_split()\n",
    "    if 'Workbook' in pages[0].metadata['source']:\n",
    "        contents.append(pages[4:])\n",
    "    else:\n",
    "        contents.append(pages[4:-1])\n",
    "        indexes.append(pages[-1])\n",
    "\n",
    "len(indexes), len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_document(page_content):\n",
    "    # Remove headers, footers, and any licensing or copyright information\n",
    "    cleaned_content = re.sub(r'©.*?SANS Institute \\d{4}.*', '', page_content, flags=re.DOTALL)\n",
    "    # Remove email addresses\n",
    "    cleaned_content = re.sub(r'\\S+@\\S+', '', cleaned_content)\n",
    "    # Normalize whitespace to single space\n",
    "    cleaned_content = re.sub(r'\\s+z', ' ', cleaned_content)\n",
    "    return cleaned_content\n",
    "\n",
    "for index in indexes:\n",
    "    index.page_content = clean_document(index.page_content)\n",
    "\n",
    "for book in contents:\n",
    "    for page in book:\n",
    "        page.page_content = clean_document(page.page_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This course was conceived and authored by David Hoelzer. David is the COO of Enclave Forensics,\\nInc., a managed security monitoring company. He also serves as Dean of Faculty for the SANS\\nTechnology Institute and a Faculty Fellow for The SANS Institute.\\nDavid has been working in the IT and Information Security fields since the late 1980s. In addition to\\ndaily work in network monitoring, analysis, and secure development, he leads the machine learning\\ninitiatives within Enclave. His particular area of focus is supervised learning solutions for real-time\\nmonitoring and classification of enterprise network activities.\\n3'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents[0][0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def split_pages(book, max_length=512):\n",
    "    \"\"\"\n",
    "    Recursively splits the content of each page in a book into chunks of no more than max_length characters.\n",
    "    Returns a list of Document objects containing the split chunks.\n",
    "    \"\"\"\n",
    "    processed_pages = []\n",
    "    for page in book:\n",
    "        cleaned_content = page.page_content.strip().replace('\\n', ' ')\n",
    "        if len(cleaned_content) <= max_length:\n",
    "            # Content is short enough, no need to split\n",
    "            processed_pages.append(Document(cleaned_content, metadata=page.metadata.copy()))\n",
    "        else:\n",
    "            # Split the content recursively\n",
    "            processed_pages.extend(split_content(cleaned_content, max_length, page.metadata))\n",
    "    return processed_pages\n",
    "\n",
    "def split_content(content, max_length, metadata):\n",
    "    \"\"\"\n",
    "    Recursively splits the given content into chunks of no more than max_length characters.\n",
    "    Returns a list of Document objects containing the split chunks, with the provided metadata.\n",
    "    \"\"\"\n",
    "    if len(content) <= max_length:\n",
    "        return [Document(content, metadata=metadata.copy())]\n",
    "    \n",
    "    # Find the last period before the max_length limit\n",
    "    last_period_idx = content[:max_length].rfind('.')\n",
    "    \n",
    "    if last_period_idx == -1:\n",
    "        # No period found, split at max_length\n",
    "        return [Document(content[:max_length], metadata=metadata.copy())] + split_content(content[max_length:], max_length, metadata)\n",
    "    else:\n",
    "        # Split at the last period and recurse on the remaining text\n",
    "        chunk = content[:last_period_idx+1]  # Include the period\n",
    "        remaining = content[last_period_idx+1:].strip()\n",
    "        return [Document(chunk, metadata=metadata.copy())] + split_content(remaining, max_length, metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = []\n",
    "for book in contents:\n",
    "    processed_data.extend(split_pages(book))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This course was conceived and authored by David Hoelzer. David is the COO of Enclave Forensics, Inc., a managed security monitoring company. He also serves as Dean of Faculty for the SANS Technology Institute and a Faculty Fellow for The SANS Institute. David has been working in the IT and Information Security fields since the late 1980s. In addition to daily work in network monitoring, analysis, and secure development, he leads the machine learning initiatives within Enclave.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(langchain_core.documents.base.Document, 3401, None)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(processed_data[0]), len(processed_data), print(processed_data[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ship docs to for text expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "ELASTIC_URL = getpass(\"Elastic URL\")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ElasticsearchStore(\n",
    "    es_url=ELASTIC_URL,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    "    index_name=\"sec595\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nc\\AppData\\Local\\Temp\\ipykernel_30552\\781378470.py:7: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
      "  vector_store.client.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n"
     ]
    }
   ],
   "source": [
    "INDEX_NAME = \"sec595\"\n",
    "\n",
    "SHOULD_DELETE_INDEX = True\n",
    "\n",
    "if SHOULD_DELETE_INDEX:\n",
    "    if vector_store.client.indices.exists(index=INDEX_NAME):\n",
    "        vector_store.client.indices.delete(index=INDEX_NAME, ignore=[400, 404])\n",
    "\n",
    "\n",
    "documents = vector_store.from_documents(\n",
    "processed_data,\n",
    "es_url=ELASTIC_URL,\n",
    "es_api_key=ELASTIC_API_KEY,\n",
    "index_name=INDEX_NAME,\n",
    "strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(\n",
    "    model_id=\".elser_model_2_linux-x86_64\"\n",
    "),\n",
    "bulk_kwargs={\n",
    "    \"request_timeout\": 60,\n",
    "},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(output):\n",
    "    print(\"Total results: \", len(output))\n",
    "    for index in range(len(output)):\n",
    "        print(output[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  4\n",
      "page_content='One of the most important is that when the system is learning, which we should clearly understand to mean automatically updating coefficients to approximate a function that minimizes loss, our network does not understand the movie reviews or the emails at all! The approach to representing our data is called Bag of Words. This name derives from the fact that we are simply tracking which words are used, completely disregarding the order that the words are in2.' metadata={'source': 'decrypted_SEC595 - Book 4_2036060.pdf', 'page': 46}\n",
      "page_content='If you consider the two following sentences, you will appreciate why this is such a big problem: You did understand Did you understand Both of these sentences contain the same words and they would be encoded identically under Bag of Words. Bag of Words also doesn’t preserve the number of times any given word appears in a piece of text. Since we are looking to preserve word order and context, we need a way to encode our data that does this. TensorFlow has just such a useful tool built into the Keras library.' metadata={'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 213}\n",
      "page_content=\"Remember that in a bag of words approach, we need to work out a dictionary that will be sorted from most common to least commonly used words. To properly build this dictionary we must have all of the words, not just the unique words in a message. Hint:When opening the file, you may find it advantageous to use the encoding='utf-8' keyword argument to force the encoding. Additionally, the errors='ignore' option can allow us to disre- gard invalid UTF sequences.\" metadata={'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 134}\n",
      "page_content='There are some downsides to this approach: •We lose the order of the words in the review •We lose detail; if a word repeats, it is only encoded one time The approach that we are taking is known as Bag of W ords. Is this a valid approach? As we know, the answer to whether an approach is valid is less relevant than whether it works.. Since there is no good way to rigorously prove that an approach is correct, we base our answer on experimental evidence.' metadata={'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 116}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents.client.indices.refresh(index=\"sec595\")\n",
    "\n",
    "results = documents.similarity_search(\n",
    "    \"Summarize Bag of Words?\", k=4, strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(\n",
    "    model_id=\".elser_model_2_linux-x86_64\"\n",
    ")\n",
    ")\n",
    "showResults(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the baseline index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "for index in indexes:\n",
    "    lines = index.page_content.split('\\n')\n",
    "    records = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for line in lines[1:]:\n",
    "        if not re.match(r'^\\d+', line):\n",
    "            records.append(line)\n",
    "            txt = line.split(',')\n",
    "            idx.append({'topic': txt[0], 'book': index.metadata['source'], 'pages': [tx.strip() for tx in txt[1:]]})\n",
    "            topic = txt[0]\n",
    "            j = int(i)\n",
    "            i += 1\n",
    "        else:\n",
    "            if j < len(records):\n",
    "                records[j] = records[j] + ' ' + line\n",
    "                txt = line.split(', ')\n",
    "                idx[j]['pages'] = idx[j]['pages'] + txt[:]\n",
    "                i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: Epsilon pages ['seeDBSCAN'] row: Elbow Method\n",
      "topic: latentspace pages ['22'] row: Huberloss\n",
      "topic: Leaky Rectified Linear Unit pages ['52', '53'] row: IMDB dataset\n",
      "topic: linear regression pages ['6'] row: IMDB dataset\n",
      "topic: normal forms pages ['63', '74', '89', '90', '94', '95'] row: MongoEngine\n",
      "topic: NumPy pages ['3', '7'] row: NetFlow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>book</th>\n",
       "      <th>pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anaconda</td>\n",
       "      <td>Book 1</td>\n",
       "      <td>[10, 11, 47, 74, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BackBlaze</td>\n",
       "      <td>Book 2</td>\n",
       "      <td>[28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Book 4</td>\n",
       "      <td>[45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayes Theorem</td>\n",
       "      <td>Book 2</td>\n",
       "      <td>[56–58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bayesian</td>\n",
       "      <td>Book 4</td>\n",
       "      <td>[45]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           topic    book                pages\n",
       "0       Anaconda  Book 1  [10, 11, 47, 74, 8]\n",
       "1      BackBlaze  Book 2                 [28]\n",
       "2   Bag of Words  Book 4                 [45]\n",
       "3  Bayes Theorem  Book 2              [56–58]\n",
       "4       Bayesian  Book 4                 [45]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This function is like 99% there. I broke my brain trying to handle the edge cases.\n",
    "# Since there are two columns, it is sometimes difficult to know when to start a new topic.\n",
    "\n",
    "df = pd.DataFrame(idx)\n",
    "df = df.explode('pages')\n",
    "# group by topic and book\n",
    "df = df.groupby(['topic', 'book'])['pages'].apply(list).reset_index()\n",
    "df['book'] = df['book'].apply(lambda x: x.split(' - ')[1].split('_')[0])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    for j, page in enumerate(row['pages']):\n",
    "        if isinstance(page, str):\n",
    "            if re.search(r'[a-zA-Z]', page):\n",
    "                if re.search(r'[0-9]', page):\n",
    "                    # Get topic for new row\n",
    "                    newtopic = re.match(r\"\\d{1,3}(.*)\", page).group(1)\n",
    "                    # Adjust value of current page to only contain the digits\n",
    "                    row['pages'][j] = re.match(r\"(\\d{1,3}).*\", page).group(0)\n",
    "                    newpages = row['pages'][j+1:]\n",
    "                    row['pages'] = row['pages'][:j-1]\n",
    "                    print(f'topic: {newtopic} pages {str(newpages)} row: {row[\"topic\"]}')\n",
    "                    dfx = pd.DataFrame({'topic': newtopic, 'book': [row['book']], 'pages': [newpages]})\n",
    "                    df = pd.concat([df, dfx], axis=0)\n",
    "                    \n",
    "# sort df by book and  topic\n",
    "df = df.sort_values(by=['book', 'topic']).reset_index(drop=True)\n",
    "# drop rows with null page\n",
    "df = df.dropna(subset=['pages'])\n",
    "# aggregate by topic & book then deduplicate the items in pages list\n",
    "df = df.groupby(['topic', 'book'])['pages'].apply(lambda x: list(set([item for sublist in x for item in sublist]))).reset_index()\n",
    "# sort the values in each row for pages even if its a str\n",
    "df['pages'] = df['pages'].apply(lambda x: sorted(x, key=lambda y: str(y)))\n",
    "# get rid of empty values in the pages list\n",
    "df['pages'] = df['pages'].apply(lambda x: [i for i in x if i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do Elastic Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "# Create a connection to Elasticsearch\n",
    "es = Elasticsearch(\n",
    "    ELASTIC_URL,\n",
    "    api_key=ELASTIC_API_KEY\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'name': 'serverless', 'cluster_name': 'ca9a9d90a7e44baab2629c85cc6ba7ea', 'cluster_uuid': 'uoMZYkwSR9auvQUhIRxtXQ', 'version': {'number': '8.11.0', 'build_flavor': 'serverless', 'build_type': 'docker', 'build_hash': '00000000', 'build_date': '2023-10-31', 'build_snapshot': False, 'lucene_version': '9.7.0', 'minimum_wire_compatibility_version': '8.11.0', 'minimum_index_compatibility_version': '8.11.0'}, 'tagline': 'You Know, for Search'})"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'metadata': {'source': 'decrypted_SEC595 - Book 4_2036060.pdf', 'page': 46}, 'text': 'One of the most important is that when the system is learning, which we should clearly understand to mean automatically updating coefficients to approximate a function that minimizes loss, our network does not understand the movie reviews or the emails at all! The approach to representing our data is called Bag of Words. This name derives from the fact that we are simply tracking which words are used, completely disregarding the order that the words are in2.'}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 134}, 'text': \"Remember that in a bag of words approach, we need to work out a dictionary that will be sorted from most common to least commonly used words. To properly build this dictionary we must have all of the words, not just the unique words in a message. Hint:When opening the file, you may find it advantageous to use the encoding='utf-8' keyword argument to force the encoding. Additionally, the errors='ignore' option can allow us to disre- gard invalid UTF sequences.\"}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 213}, 'text': 'If you consider the two following sentences, you will appreciate why this is such a big problem: You did understand Did you understand Both of these sentences contain the same words and they would be encoded identically under Bag of Words. Bag of Words also doesn’t preserve the number of times any given word appears in a piece of text. Since we are looking to preserve word order and context, we need a way to encode our data that does this. TensorFlow has just such a useful tool built into the Keras library.'}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 213}, 'text': 'Previously, we used the Bag of W ords approach, generating a multi-hot encoded vector indicating which words were present in a given text. One of the major limitations of this approach was that we only know that a word was present, not the order that the words appeared in.'}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 132}, 'text': '4.03 BOW Ham vs Spam F ebruary 10, 2023 1 Bag of W ords: Ham versus Spam 1.1 Overview In this lab, we will attempt to take the lessons learned from the IMDB classification using bag of words and apply them to a binary classification of ham versus spam. 1.2 Goals By the end of this lab, you should be able to: •Implement a bag of words approach for a binary regression •Understand the fundamentals of tuning the hyperparameters for a model •Understand the roll of batch size and epochs 1.'}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 116}, 'text': 'There are some downsides to this approach: •We lose the order of the words in the review •We lose detail; if a word repeats, it is only encoded one time The approach that we are taking is known as Bag of W ords. Is this a valid approach? As we know, the answer to whether an approach is valid is less relevant than whether it works.. Since there is no good way to rigorously prove that an approach is correct, we base our answer on experimental evidence.'}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 150}, 'text': '19 Conclusion This lab covered a lot of ground! We have several key takeaways that you will be expected to apply in future labs: •How to apply the Bag of Words approach using multi-hot encoding and a dense network •What the impact of the batch size is on the training process •How to use the built in evaluate() method to test a model149'}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 206}, 'text': '9435 Those numbers don’t look as good as they did when we used Bag of Words with only Dense layers! Your validation accuracy likely tops out with the CNN at about 95%. What could be happening? 10 T ask 2.8 Plot the training history for your model. [8]:plot_history(training_history .history)205'}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 130}, 'text': 'This network wasn’t very deep, but even so, it was able to perform pretty well on a task that would normally require a human! The evaluate() function tells us that our test data performs just slightly worse than our validation accuracy predicted that it would. At a higher level, we also learned: •What the Bag of Words approach is and how to apply it •How to vectorize data through multi-hot encoding •How to build a sequential model •How to train a TensorFlow model •How to generate predictions129'}\n",
      "{'metadata': {'source': 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'page': 109}, 'text': '2 Goals By the end of this lab, you should be able to: •Understand and use Multi-Hot Encoding •Create a TensorFlow model using dense layers •Understand what the Bag of W ords approach is and how it is applied •See the difference between “Machine Learning” and “Understanding” 1.3 Estimated Time: 45 - 60 minutes 2 Importing the Libraries In this lab, we will use a toy dataset that is included as a part of the Keras package. What is a toy dataset? Don’t let the name fool you.'}\n"
     ]
    }
   ],
   "source": [
    "model = '.elser_model_2_linux-x86_64'\n",
    "# Perform a query\n",
    "query = {\n",
    "    \"_source\": {\"includes\": [\"metadata*\", \"text\"]},\n",
    "   \"query\":{\n",
    "      \"text_expansion\":{\n",
    "         \"vector.tokens\":{\n",
    "            \"model_id\":model,\n",
    "            \"model_text\":\"What is bag of words\"\n",
    "         }\n",
    "      }\n",
    "   }\n",
    "}\n",
    "\n",
    "# Search the index\n",
    "response = es.search(index=INDEX_NAME, body=query, )\n",
    "\n",
    "# Process the response\n",
    "for hit in response['hits']['hits']:\n",
    "    print(hit['_source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'took': 29, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def generate_word_cloud(text):\n",
    "    wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "for content_text in contents:\n",
    "    generate_word_cloud(' '.join([page.page_content for page in content_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facilitating the review process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation & Parsing of the PDF Files\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
