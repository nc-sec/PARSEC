{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P.A.R.S.E.C. - PDF Analysis and Review System for Exam Content\n",
    "\n",
    "This Jupyter Notebook, titled \"P.A.R.S.E.C. - PDF Analysis and Review System for Exam Content\", aims to provide a system for analyzing and reviewing PDF documents related to exam content. The notebook will include various functionalities such as parsing PDF files, extracting text and metadata, performing text analysis, generating visualizations, and facilitating the review process. The goal is to create an efficient and comprehensive system for working with exam-related PDF documents.\n",
    "\n",
    "## Requirements\n",
    "python 3.10.11\n",
    "Elastic Cloud instance w/ ELSER Model\n",
    "nvidia gpu or llm api key\n",
    "\n",
    "# Optional\n",
    "Langsmith Account/API key (free tier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting jq\n",
      "  Using cached jq-1.7.0.tar.gz (2.0 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Building wheels for collected packages: jq\n",
      "  Building wheel for jq (pyproject.toml): started\n",
      "  Building wheel for jq (pyproject.toml): finished with status 'error'\n",
      "Failed to build jq\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × Building wheel for jq (pyproject.toml) did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [5 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_ext\n",
      "      Executing: ./configure CFLAGS=-fPIC -pthread --disable-maintainer-mode --with-oniguruma=builtin\n",
      "      error: [WinError 2] The system cannot find the file specified\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for jq\n",
      "ERROR: Could not build wheels for jq, which is required to install pyproject.toml-based projects\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "#!python3 -m pip install -qU elasticsearch langchain langchain-elasticsearch openai tiktoken PyPDF4\n",
    "\n",
    "#%pip install PyPDF4\n",
    "#%pip install elasticsearch\n",
    "#%pip install langchain\n",
    "#%pip install langchain-elasticsearch\n",
    "#%pip install openai tiktoken\n",
    "#%pip install wordcloud\n",
    "#%pip install rapidocr-onnxruntime\n",
    "#%pip install nltk\n",
    "%pip install jq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'jq'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmultiprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Pool\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjq\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Download the Punkt tokenizer models (only needed once)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m#nltk.download('punkt')\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'jq'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "from getpass import getpass\n",
    "from urllib.request import urlopen\n",
    "from pypdf import PdfReader, PdfWriter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_elasticsearch import ElasticsearchStore\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "#from langchain_community.document_loaders import JSONLoader\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import jq\n",
    "\n",
    "\n",
    "# Download the Punkt tokenizer models (only needed once)\n",
    "#nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparation of the PDF Files\n",
    "Get rid of pesky passwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['decrypted_SEC595 - Book 1_2036060.pdf', 'decrypted_SEC595 - Book 2_2036060.pdf', 'decrypted_SEC595 - Book 3_2036060.pdf', 'decrypted_SEC595 - Book 4_2036060.pdf', 'decrypted_SEC595 - Book 5_2036060.pdf', 'decrypted_SEC595 - Book 6_2036060.pdf', 'decrypted_SEC595 - Workbook 1_2036060.pdf', 'decrypted_SEC595 - Workbook 2_2036060.pdf', 'SEC595 - Book 1_2036060.pdf', 'SEC595 - Book 2_2036060.pdf', 'SEC595 - Book 3_2036060.pdf', 'SEC595 - Book 4_2036060.pdf', 'SEC595 - Book 5_2036060.pdf', 'SEC595 - Book 6_2036060.pdf', 'SEC595 - Workbook 1_2036060.pdf', 'SEC595 - Workbook 2_2036060.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "pdf_files = [f for f in os.listdir(cwd) if f.endswith('.pdf')]\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader, PdfWriter\n",
    "i = 0\n",
    "for pdf_file in pdf_files:\n",
    "    if pdf_file.startswith('decrypted_'):\n",
    "        continue\n",
    "    with open(pdf_file, 'rb') as file:\n",
    "        print(pdf_file, ' is decrypting')\n",
    "        reader = PdfReader(file)\n",
    "        # Attempt to decrypt the PDF with an empty password\n",
    "        if reader.is_encrypted:\n",
    "            try:\n",
    "                reader.decrypt('PpH[uQ(7+Gy:FdA9;X9QVXi@$zVwD-') # Your password here\n",
    "            except:\n",
    "                print(\"The PDF is encrypted and cannot be decrypted with the password.\")\n",
    "                exit()\n",
    "        \n",
    "        writer = PdfWriter()\n",
    "        \n",
    "        # Copy the content from the original PDF to the new PDF\n",
    "        for page_num in range(len(reader.pages)):\n",
    "            page = reader.pages[page_num]\n",
    "            writer.add_page(page)\n",
    "        \n",
    "        # Save the new PDF file without encryption\n",
    "        with open('decrypted_'+pdf_files[i], 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "        i += 1\n",
    "        print(\"The PDF has been successfully decrypted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting text and metadata\n",
    "We'll load the decrypted pdfs, split pages, and so on.\n",
    "There are books and workbooks within the provided materials and they both require some cleaning. Basically everything after the SANS copyright is useless. For each book we'll take two slices of pages, [4:-1] & [-1]. The former is the course content and the latter is the provided index. For workbooks, we only need one slice [4:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdfs = [f for f in os.listdir(cwd) if f.startswith('decrypted_')]\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "indexes = []\n",
    "contents = []\n",
    "for pdf in pdfs:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    pages = loader.load_and_split()\n",
    "    if 'Workbook' in pages[0].metadata['source']:\n",
    "        contents.append(pages[4:])\n",
    "    else:\n",
    "        contents.append(pages[4:-1])\n",
    "        indexes.append(pages[-1])\n",
    "\n",
    "len(indexes), len(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_document(document):\n",
    "    # Remove headers, footers, and any licensing or copyright information\n",
    "    cleaned_content = re.sub(r'©.*?SANS Institute \\d{4}.*', '', document, flags=re.DOTALL)\n",
    "    # Remove email addresses\n",
    "    cleaned_content = re.sub(r'\\S+@\\S+', '', cleaned_content)\n",
    "    # Normalize whitespace to single space\n",
    "    cleaned_content = re.sub(r'\\d$', '', cleaned_content)\n",
    "\n",
    "    return cleaned_content\n",
    "\n",
    "\n",
    "for index in indexes:\n",
    "    index.page_content = clean_document(index.page_content)\n",
    "\n",
    "for content in contents:\n",
    "    for page in content:\n",
    "        page.page_content = clean_document(page.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets slice a bit more & index the contents in elastic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1318.1304347826087,\n",
       " 1089.3152173913043,\n",
       " 1117.5348837209303,\n",
       " 1093.0131578947369,\n",
       " 853.34375,\n",
       " 1043.159090909091,\n",
       " 1373.6743119266055,\n",
       " 1587.4894894894894]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get average length of the pages for each book\n",
    "avg_page_lengths = []\n",
    "\n",
    "for content in contents:\n",
    "    avg_page_lengths.append(sum([len(page.page_content) for page in content]) / len(content))\n",
    "\n",
    "avg_page_lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, list)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sec595), type(contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec595 = contents.copy()\n",
    "\n",
    "for i, book in enumerate(sec595):\n",
    "    for j, page in enumerate(book):\n",
    "        sec595[i][j].page_content = page.page_content.split('.\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I wanted to concatenate some stuff \n",
    "# where the section was too short\n",
    "# or too long!\n",
    "\n",
    "```py\n",
    "def process_page_content(page):\n",
    "    # Use a temporary list to hold new or modified content to avoid frequent list modifications\n",
    "    new_content = []\n",
    "    for k, sect in enumerate(page.page_content):\n",
    "        if len(sect) > 1000:\n",
    "            # Efficiently split by sentences and divide\n",
    "            split = sect.split('. ')\n",
    "            half = len(split) // 2\n",
    "            sect1 = '. '.join(split[:half])\n",
    "            sect2 = '. '.join(split[half:])\n",
    "            new_content.append(sect1)\n",
    "            new_content.append(sect2)\n",
    "        elif len(sect) < 75 and k > 0:\n",
    "            # Instead of removing, just append to the previous section if possible\n",
    "            new_content[-1] += ' ' + sect\n",
    "        else:\n",
    "            new_content.append(sect)\n",
    "    return new_content\n",
    "\n",
    "def process_book(book):\n",
    "    for j, page in enumerate(book):\n",
    "        book[j].page_content = process_page_content(page)\n",
    "    return book\n",
    "    ```\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=['This course was conceived and authored by David Hoelzer. David is the COO of Enclave Forensics,\\nInc., a managed security monitoring company. He also serves as Dean of Faculty for the SANS\\nTechnology Institute and a Faculty Fellow for The SANS Institute', 'David has been working in the IT and Information Security fields since the late 1980s. In addition to\\ndaily work in network monitoring, analysis, and secure development, he leads the machine learning\\ninitiatives within Enclave. His particular area of focus is supervised learning solutions for real-time\\nmonitoring and classification of enterprise network activities '], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 4}),\n",
       " Document(page_content=['Introduction\\nThis course is broken down into six major sections, each of which corresponds to the coursebook for\\nthat day. When undertaking this course, please have in mind that we view the first two sections of\\nmaterial as important foundation material that students should already have some basic familiarity\\nwith', 'Since we recognize that different students will have different levels of competence with topics such\\nas Python, SQL, document stores, statistics, and mathematical signals analysis, the first two books\\nserve as a crash-course / refresher for these topics. While you will almost certainly have some level\\nof mastery of one or more of these topics (or at least mastery that exceeds the requirements of this\\ncourse), it is equally likely that one or more of these topics will not be familiar', 'Whatever your level of experience, we invite you to dive in and enjoy the material. It is important\\nthat you come into the class prepared to work through nearly half of the material on your own in\\nhands-on exercises. You must also be prepared to go beyond what the labs require, experimenting\\nwith the concepts and techniques to work toward mastery and application toward your own business\\nproblems '], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 5}),\n",
       " Document(page_content=['Even though this class is a week in length, there are limitations to what we can and cannot accom-\\nplish. To set your expectations appropriately, let’s explain what we willdo and what we will not do\\nthis week', 'There is just no way we can realistically take someone from zero to a data scientist in just one\\nweek. To do so, we would have to require significant prerequisites in mathematics, statistics, and\\ncomputer science', 'More specifically, the computer science knowledge required would be more than\\nthe general theory taught in an undergraduate program. Instead, we would have to require that you\\nhad already mastered Python, had done significant work as a DBA using SQL based databases, you\\nhad experience modern NoSQL style document stores, and that you were also competent at scraping\\nwebsites… and those are just the computer science-based requirements!\\nWhile significantly more mathematics would be useful as a foundation for a data scientist, there’s\\njust no way that we can cover everything from statistics, algebra, probability theory, calculus (up\\nto and including differential equations), and linear algebra. Just looking at that list, it’s clear that\\nthere are multiple full semesters, if not years, of college courses required', 'Since we know we cannot require all of the foregoing, and we humbly admit that there’s no way we\\ncan possibly teach all of that in just a few days, we have instead set our sights somewhat lower', 'While we will not rigorously cover statistics or mathematics, we have carefully selected key topics\\nfrom these fields that you really must be familiar with. Similarly, we have selected a subset of the\\nmost useful and regularly used computer science disciplines that would be useful for a data scientist', 'When we turn our attention to building real machine learning networks, we again cannot possibly\\ncover all of the incarnations of such things', 'Knowing this, then, we will strive to provide not rigor but important intuitions. This isn’t to say that\\nthere is no rigor at all. We strongly believe that understanding why something works is incredibly\\n'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 6}),\n",
       " Document(page_content=['important when it comes to developing intuitions. We will, therefore, dive into some of these topics\\nto some significant depth. However, our goal in doing so is always (and only) to allow you to\\nunderstand it just well enough to be able to intuitively apply these tools and techniques to your own\\nreal-world problems', 'In fact, we can say that that is a major goal of the class. That is, to work to build a box of tools that\\nyou feel comfortable reaching into to acquire, explore, analyze, and extrapolate meaning from data '], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 7}),\n",
       " Document(page_content=['It’s reasonable to wonder how this class differs from courses or tutorials that you can find in other\\nplaces. YouTube, for example, is chock full of machine learning demonstrations and tutorials. Sim-\\nilarly, with the evolution of online course delivery, there are excellent paid and free options for\\nrigorous machine learning and artificial intelligence courses. What value does this course hold in\\nthe context of these other offerings?\\nIn our opinion, the courses, tutorials, and demonstrations that are publicly available tend to exist at\\nthe extremes of this space. These courses will either be extremely rigorous or extremely high-level Let us explain', 'The rigorous classes can be quite excellent. However, they approach this problem from an academic\\npoint of view rather than a real-world application point of view. As a result, these courses rarely, if\\never, concern themselves with how to apply machine learning to problems. Instead, they are focused\\non the mathematics required to implement a neural network, for example. Based on these founda-\\ntions, these courses will typically assist you in building your own implementation of a machine\\nlearning library. Notice that this is a library that you could use to solve real-world problems, but\\nsince these courses almost never talk about how to acquire, preprocess, and transform your data or\\nhow to go about determining a useful network design for learning, students successfully completing\\nthese courses are often stumped as to how to proceed', 'At the other extreme are the tutorial or demonstration style courses or series. These can be found in\\nplaces such as YouTube, Udemy, and Coursera. If you have ever watched anything about machine\\nlearning, it is likely one of these. While the demonstration will usually cover some specific machine\\nlearning problem, such as identifying handwritten digits, and the solution demonstrated works well,\\nthese tutorials never tell you how to preprocess your own data, how to obtain your own data, how\\nto represent your data in a useful way, why the network is built as it is, what the activation functions\\ndo (or how you select them), etc '], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 8}),\n",
       " Document(page_content=['This course is quite different. We have made an effort to position this course squarely in the center\\nof these extremes while simultaneously attempting to provide you the tools that you need to go out\\nand get your own data and turn that data into something that a machine learning model can process', 'In order to meet the lofty goals of assisting you to develop familiarity with a set of tools in your\\nmachine learning toolbox, being able to apply these tools on your own, the ability to extract and\\ntransform data in a way that machine learning can be applied, and the ability to create your own\\nuseful machine learning models, we have settled on an experiential approach. What does this mean\\nin terms of course design?\\nWith the exception of this introduction section and the following terms and technologies introduc-\\ntion, each “lecture” portion of the class will be covered within 30 to 60 minutes. These lectures\\nmight be lectures, some are discussions, and many include demonstrations. Following each of\\nthese sections, you will have guided hands-on activities, each of which will take another 30 to 60\\nminutes', 'The exercises are almost entirely performed using Jupyter Lab through Anaconda Navigator, which\\nwe will get you to install on your system when we get to our first exercise. We have provided you\\na zip file containing all of the data and notebooks used in this course. The notebooks come in pairs', 'The first is the guided lab notebook that you should work through, striving to complete each of the\\nrequired activities. The second is an identical notebook with solutions in it. We urge you to use the\\nfirst notebook and refer to the second only when you are stuck!\\nWe have also provided a single virtual machine. Some of our exercises this week make use of this\\nVM. This is especially true for labs that require you to acquire data from some type of database or\\npossibly from some other common information source that you might find in your own environment We will work on getting this VM up and running during our first lab', 'One of the most important things to keep in mind as you work through this class is that you should\\nwork to master topics as they come. Do not be satisfied if there’s something covered in the first lab\\nthat you don’t quite understand. If we cover it in the first lab, you can be almost certain that we will\\nrely on your mastery of that knowledge in many, if not all, of the labs that follow!\\nTherefore, if you are stuck on something or if something isn’t clear, please ask for help! If you\\nare taking this course at a live venue (or live online), ask your instructor or a TA to come over and\\nhelp you out. Put your hand up and ask questions! If you are taking this class OnDemand, you can\\nalways reach out to the SANS lab support team through the portal '], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 9}),\n",
       " Document(page_content=['Technologies & Terms\\nWith our introduction out of the way, it’s time for us to get started with the first introductory topic in\\nour course. At this point in the course, we are not going to attempt to cover all of the technologies\\nand terms deeply. Nearly everything mentioned here is covered in much greater depth later in the\\ncourse', 'Why are we taking the time to talk about this now, then? First, to provide you with an overall view\\nof where we are going in this course. Second, to provide a common framework of terms that we\\nwill rely on for the rest of the class', 'There are certainly more terms that we will learn as we go. This base set of terms connected with\\nsome fundamental skills with the technologies we will use to learn about machine learning should\\nmake the first two days a bit easier to digest '], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 10}),\n",
       " Document(page_content=['Tools Overview\\nJupyter Notebook has become one of the most commonly used tools for interactive Python develop-\\nment that allows for easy experimentation, simple inline documentation, inline visualization, and\\ncode organization. While the tool is perhaps best known for its use with Python for machine learning\\nand data science tasks, it supports many more languages', 'The supported languages include R, SQL, Mongo (as far as it can be viewed as a language), C, Ruby,\\nand many more. The support for these is through modules termed kernels . Since Jupyter supports\\nall of the languages and technologies that we make use of in this course, it provides a natural and\\nobvious choice for our labs. Another huge bonus is the ability to create inline documentation via\\nMarkdown. This allows us to build what amounts to an interactive workbook', 'The particular version of Jupyter that we will be using is Jupyter Lab, which is the most recent in-\\ncarnation. We will walk through installing the Anaconda Dashboard during our first lab. Anaconda\\nwill make working with Jupyter very simple', 'This choice is also motivated by the desire to create an experience that you can replicate in your\\noffice and which mirrors what many data scientists use every day. We will be using Anaconda as an\\neasy way to get Jupyter, Python, and some other required libraries installed on your system, which\\nwill allow you to translate this directly to your work environment', 'For example, while we do not use Anaconda within our office as the basis for our research server, we\\ndo use Jupyter Lab. Jupyter Lab allows us to have a shared workspace with shared notebooks while\\nalso allowing us to have some basic security. Why not just use Jupyter on individual workstations?\\nThere are a few main reasons', 'The first reason is a concern about continuity. We do not back up workstations, period. By providing\\n1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 11}),\n",
       " Document(page_content=['a centralized Jupyter Lab server, we can create a server with plenty of redundant storage that is\\nbacked up frequently', 'The second reason is far more compelling. Rather than investing thousands of dollars per worksta-\\ntion for additional RAM, high-end GPUs, storage, etc., we can put all of that money into a single\\nsystem that has 512 gigs of RAM, 16 terabytes of storage, and as many GPUs as the PCI-e channels\\ncan handle. While this single system is pricey, we end up utilizing the hardware far more exten-\\nsively than we would 15 high-end workstations, most of which would sit idle unless the user was\\nactively working on a data science task', 'Note that we do not use Anaconda on that server. It’s not Anaconda that makes this all work. We\\nhave simply installed all of the necessary tools and supporting libraries, including Jupyter. Ana-\\nconda just makes that installation easy, especially since we’re trying to support whatever kind of\\ncomputer a student happens to show up to class with!\\n1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 12}),\n",
       " Document(page_content=['The primary language that we will use for our activities in this class is Python. There are other\\nlanguages frequently used in data science. R is one of the most popular, Julia is another common\\noption, and Go (or Golang) has some data science adherents as well', 'Why have we chosen Python over the other options? This is a fair question, especially since R and\\nJulia are domain-specific languages designed for data analytics and statistics. The answer to that\\nquestion, however, actually lies in that last sentence', 'R and Julia are domain-specific languages. A domain-specific language is a language created for\\none very specific task or set of tasks. This tends to mean that the language is highly optimized\\nfor those tasks and should allow you to solve problems in that domain space intuitively using the\\nlanguage. The downside of this is that these languages are difficult to use to do anything else', 'If you already use one of these languages, don’t take this the wrong way. We’re not implying that\\nthere’s anything wrong with these languages, nor are we challenging you to solve general-purpose\\ncomputing problems with them. By comparison, however, for people who work in the information\\nsecurity or information technology spaces, Python makes much more sense', 'Another big downside to R and Julia is that they do not lend themselves to building finished solutions', 'For example, if we spent a few weeks exploring, transforming, and experimenting with our data in\\nR and then wanted to integrate that into some production system, we would need to reproduce all\\nof that work in some other language to easily connect it to our web service, for example', 'Python, on the other hand, has had rapidly growing support for statistics and analysis libraries', 'While the support is not yet as robust as it is for a language like R, we have the added advantage\\nof Python being familiar. There’s a reasonable chance that you have used Python and will continue\\nto use Python for many other tasks in your environment. Finally, Python is easily integrated into\\nalmost any process that you have since it isa general-purpose language 1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 13}),\n",
       " Document(page_content=['When it comes to putting our machine learning theory into practice, we will use TensorFlow through-\\nout this class. We recognize that there are many researchers who have strong loyalties and prefer-\\nences for PyTorch. Our decision, however, is not arbitrary', 'PyTorch is a free open-source machine learning library. It is a Python-based implementation of\\nTorch, which is a Lua wrapper around a compiled C machine learning library. PyTorch is actively\\nsupported by Facebook through the Facebook AI Research lab (FAIR). It will most frequently be\\nfound within academic courses that feature or develop machine learning solutions. You may also\\nfind it in a lesser proportion of commercial applications. A nice bonus to PyTorch is that AWS has\\nbuilt-in support for it, but PyTorch can be very challenging to use with other languages. In fact, to\\ndo so, you almost always write a Python wrapper that is called by your other language', 'TensorFlow, on the other hand, is a free open-source product from Google. Whereas PyTorch is\\nfound in a small proportion of commercial machine learning solutions, TensorFlow is very widely\\nused. TensorFlow has the added advantage of being a library that you can use from Python or with\\nwhich you can directly interface using several mobile development platforms and languages like\\nSwift and C++, all without a Python wrapper 1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 14}),\n",
       " Document(page_content=['Another library that we will make heavy use of is NumPy. NumPy arrays are relied on heavily by\\nTensorFlow (and PyTorch). This proves to be fortuitous since we will begin using NumPy in the\\nlabs for books one and two since it provides excellent statistics and analysis functions', 'If you have used Python somewhat extensively already but have not used NumPy, you might wonder\\nwhy it’s needed. After all, can’t we just implement whatever functions NumPy has directly in\\nPython?\\nWhile we could do this, there are two reasons why we shouldn’t. First, we should reuse well-written\\ncode, and NumPy definitely falls into this category. Second, NumPy is actually a set of compiled\\nbinary libraries with exposed Python interfaces. This means that NumPy will perform orders of\\nmagnitude more quickly than native Python code that implements the same functionality', 'We will begin to explore NumPy later in this book. Especially if you are already a Python user,\\nbe prepared for it to be very familiar yet different. Personally, you might relate it to the difference\\nbetween Dutch and German. If you speak either language and hear the other, it sounds very familiar,\\nand you feel as though you should understand what’s being said, but things are just different enough\\nthat you need some translation 1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 15}),\n",
       " Document(page_content=['There is another library that we will use that has graphing capabilities, but it is not primarily the\\ngraphing capabilities that make it attractive for our use: Pandas. Pandas is a data analysis library It is implemented as an extension of two-dimensional NumPy arrays', 'The beauty and utility of Pandas will become apparent as we start to work with our Jupyter Note-\\nbooks. As powerful as NumPy is, it can be somewhat cumbersome when it comes to examining the\\nraw data. For example, a NumPy\\\\index{Python!NumPy array can easily handle ten thousand or a\\nmillion elements… but if we try to look at all of those elements, there are a few problems', 'First, trying to print all of those values into a Jupyter cell will be very time-consuming. Second, the\\ndata is just sort of plopped out into the cell. It doesn’t look very good, and it’s not very friendly to\\nwork with', 'Pandas, on the other hand, will allow us to represent our data in a DataFrame. The DataFrame class\\nwill allow us to look at our data in a much more appealing and easy-to-read format, in addition to\\nbeing much friendlier with Jupyter rendering', 'The value of Pandas doesn’t end here, though. It also has some super nice and easy-to-use features\\nfor visualizing correlations and other analysis of our data rapidly', 'While we will introduce and use some of the features in Pandas, be clear that we will prefer to use\\nNumPy in this course. This is a considered and intentional choice. While Pandas is a wonderful tool\\nfor rapid data manipulation, it is limited to handling two dimensional data, or ℝ2data. A number of\\nprojects that we work with will have deeper dimensional spaces, making Pandas a very challenging\\nchoice. Rather than focus on Pandas and then be required to learn the intricacies of NumPy as well,\\nwe will focus on NumPy and use Pandas for some convenience in some of our problems 1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 16}),\n",
       " Document(page_content=['It will frequently be very useful for us to be able to visualize our data in some way. This is something\\nthat we often experiment with when exploring data, seeking to get a feel for what the data contains\\nand, perhaps, relationships or patterns within the data. It is not unusual to want to create some\\nvisual representation of the data or of our analysis of that data as an end product. For example, we\\ncould create a numeric representation of the average network load in our enterprise, but a visual\\nrepresentation of that same data might be much more intuitive for the consumers of that report', 'The majority of our visualization needs will be met through the use of Matplotlib. This open-source\\nvisualization library has nearly any feature or type of graph that you might want to create. Unfor-\\ntunately, using this library is not always especially intuitive. To help with this, we have several\\nintroductory exercises in connection with this book to get you started with some common patterns', 'We will leverage this knowledge in the coming sections of the course as we explore, analyze, and\\nvisualize our data', 'While we could create finished visualizations that will be consumed by end-users of our solutions\\nusing Matplotlib, it isn’t always the best choice. To make our lives just a bit easier, we will have a\\nlook at Dash. Dash makes use of an alternative graphing library from Plotly. It is specifically tar-\\ngeted at creating web-based dashboards simply. Conveniently, there is also a Dash kernel available\\nfor Jupyter (JupyterDash) 1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 17}),\n",
       " Document(page_content=['A critical ability of a data scientist is not only the ability to transform data but to acquire data from\\nvarious sources. This will require, maybe not mastery, but indeed some facility with the languages\\nand tools used to access data', 'Traditionally, much of our data is stored in SQL databases. These relational databases are very\\npowerful tools for organizing and retrieving our data. They strike a balance between efficient stor-\\nage of the data, attempting to minimize duplication within the data, and speed of access through\\nwell-thought-out indexing and queries', 'While we will cover ANSI SQL generally, the SQL server that we have available on the course VM\\nis PostgreSQL. You will find that every SQL server has its own quirks and extra features that are\\nunique to that specific server. While the query language that we will work with is standard, looking\\nat the documentation for the specific SQL server that you have can be very valuable when you are\\ntrying to do anything more than simple queries 1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 18}),\n",
       " Document(page_content=['An alternative to the SQL paradigm is a document store. These come in a variety of forms that we\\ncan put into just two main categories. We are not saying that there are no other ways to store data,\\nhowever! For example, even though ISAM (Indexed Sequential Access Method), VSAM (Virtual\\nStorage Access Method), and others are still alive and well, they are not commonly exposed directly\\nfor queries', 'Document stores are typically either as a general document store or as a key-value store. In a sense,\\nall document stores are a form of a key-value store', 'A key-value store supports high-speed retrieval of records through a key. You can think of this key\\nas representing what we might think of as the ID column in a SQL database. With a key-value store,\\nhowever, you may never use any part of the value in your search or query. In other words, we say\\nthat the value is opaque ', 'Conversely, a general document store will also have at least one key field (though we can create\\nindexes based on other fields if we choose), and we can also specify values within the documents\\nthat we wish to include as a part of our search criteria', 'Why would these be used over a SQL store? Key-value stores are very fast, though the limitation\\nof only searching based on the key can be limiting in many applications. Document stores are also\\nfast, but the real benefit is that a single relatively fast query will return the entire document with\\nall related data since it is all stored as a single document. A SQL database can be fast, but the real\\nbenefits come from the deduplication of data, which saves space and also makes updating relatively\\neasy 1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 19}),\n",
       " Document(page_content=['Other Data Science Terms\\nAnother technology that you will run into in the data science and analytics space is Big Data. This\\nis a somewhat nebulous term that takes in many technologies, including IBM’s partnership with\\nCloudera for large-scale cloud-based mass storage, Apache Spark, which provides a solution for\\non-premises large-scale data storage, and others', 'These technologies are generally split into two main sets of features. One is the storage mechanism,\\nwhether it is using a widely distributed filesystem or some other approach. The other is the interface\\nor language that you use to interact with the data in the data lake', 'These tools typically have machine learning features built-in, but in our experience, many organi-\\nzations are not fully leveraging these capabilities. More commonly, we see them using the ETL\\ncapabilities', 'ETL stands for Extract, Transform, and Load. For example, one customer uses a Spark job to export\\nall data relating to all building projects worldwide from all of the various SQL servers and database\\ninstances in all of their data centers every night. This data is exported into a Spark Hadoop backed\\nfilesystem. Once this is done, they run a series of transformations against this data, reprocessing it\\nto generate useful analytics. Once this is completed, the resulting data is loaded into another SQL\\ndatabase server from which the analytics data is then made available through Microsoft Power BI', 'We will not be covering any of these solutions or tools in our course. Everything that we cover can\\nbe translated directly into an environment using these tools. In a very real sense, the work that we\\nare doing in this course is the precursor to being able to leverage these tools well', 'Another technology that we will touch on but not use extensively is that of MapReduce. Still, we\\nfelt that it would be important for you to be familiar with the term since it comes up frequently,\\n1'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 20}),\n",
       " Document(page_content=['especially when dealing with very large data sets', 'MapReduce is a generalized approach where we take a transformation that we would like to perform\\non a very large data set and break it down into a number of tasks that can all be run in parallel', 'The “Map” phase involves iterating over all of the data, performing the initial transformation or\\naggregation. The reduce phase takes all of the results from the map phase and aggregates them\\ndown to either a single value or a single data vector', 'A simple to grasp example would be determining the word count for every individual word in the\\n60,000+ books stored in Project Gutenberg. While the task itself is not difficult, this will require\\ntime to complete if we process each book sequentially. How could we improve this?\\nWhat if we create a map layer with 1,000 nodes in it. Each node is assigned 60 books to download\\nand count words in. All 1,000 nodes can work in parallel. Once these nodes complete, we could\\npush their individual counts through a “Reduce” layer. If we pushed them all to one node, that would\\ncreate another bottleneck, so we instead put them through a set of nodes, which then proceeds to\\nreduce further to another layer, and perhaps another, finally producing a single result 2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 21}),\n",
       " Document(page_content=['Chances are that, like most people, you come to data science with some preconceptions when it\\ncomes to some of the words that are used in this space. Take, for example, data', 'In the context of data science, data is just some fact or group of facts with no specific structure or\\ncontext. This can seem very Zen and a bit counterintuitive. After all, we are humans, and we are\\nconstantly processing facts to arrive at conclusions. We just do this naturally. Sometimes, though,\\nit does lead to incorrect conclusions', 'We will view data as facts. Data has no meaning on its own. It must be interpreted in some way 2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 22}),\n",
       " Document(page_content=['When we take a collection of facts, transform them (perhaps) in some way, and aggregate them,\\nthese facts can be interpreted or structured to produce information. Information has meaning and\\nassists us in making decisions', 'Using the example that began on the previous slide, imagine that we move our foot and feel it\\ntouching something. Perhaps you can relate to this more with the idea of your hand or fingers\\ntouching something unexpected. We can react without a great deal of thought, immediately jumping\\nto a conclusion without doing a lot of processing', 'For example, if we feel something round and long under our foot that seems to move, we might\\nimmediately react with the thought, “Snake!” But is it a snake? More facts would be required. If\\nthe only facts we have are long, round, and moving, we might also be stepping on a branch or the\\ntail of our cat! It’s only after we analyze as many facts as possible that we can make an accurate\\nassessment', 'Admittedly, when your hand touches something unexpected or limited data makes your brain think,\\n“Snake!” you are probably happy to seem a little bit foolish in the case of “not snake”. Imagine\\nhow relieved you would be if you discovered that it wasa snake!\\nA well-known illustration, based on a poem by John Godfrey Saxe named Blind Men and the Ele-\\nphant1, is useful for thinking about the definition of information that we are discussing. While the\\npoem is really about the opinions that scholars and others form on topics about which they know\\nnext to nothing, the idea still works well here:\\nIt was six men of Indostan\\nTo learning much inclined,\\nWho went to see the Elephant\\n1https://allpoetry.com/The-Blind-Man-And-The-Elephant\\n2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 23}),\n",
       " Document(page_content=['(Though all of them were blind),\\nThat each by observation\\nMight satisfy his mind', 'The First approached the Elephant,\\nAnd happening to fall\\nAgainst his broad and sturdy side,\\nAt once began to bawl:\\n\"God bless me!—but the Elephant\\nIs very like a wall!\"\\nThe Second, feeling of the tusk,\\nCried: \"Ho!—what have we here\\nSo very round and smooth and sharp?\\nTo me \\'t is mighty clear\\nThis wonder of an Elephant\\nIs very like a spear!\"\\nThe Third approached the animal,\\nAnd happening to take\\nThe squirming trunk within his hands,\\nThus boldly up and spake:\\n\"I see,\" quoth he, \"the Elephant\\nIs very like a snake!\"\\nThe Fourth reached out his eager hand,\\nAnd felt about the knee', '\"What most this wondrous beast is like\\nIs mighty plain,\" quoth he;\\n\"\\'T is clear enough the Elephant\\nIs very like a tree!\"\\nThe Fifth, who chanced to touch the ear,\\nSaid: \"E\\'en the blindest man\\nCan tell what this resembles most;\\nDeny the fact who can,\\nThis marvel of an Elephant\\nIs very like a fan!\"\\nThe Sixth no sooner had begun\\nAbout the beast to grope,\\nThan, seizing on the swinging tail\\nThat fell within his scope,\\n\"I see,\" quoth he, \"the Elephant\\nIs very like a rope!\"\\n2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 24}),\n",
       " Document(page_content=['And so these men of Indostan\\nDisputed loud and long,\\nEach in his own opinion\\nExceeding stiff and strong,\\nThough each was partly in the right,\\nAnd all were in the wrong!\\nSo, oft in theologic wars\\nThe disputants, I ween,\\nRail on in utter ignorance\\nOf what each other mean,\\nAnd prate about an Elephant\\nNot one of them has seen!\\nWhile this is a commentary on opinions and bias, the idea is useful. Information, for us, is created\\nby aggregating many data points. Each of those data point represents some fact which, on its own,\\nmeans very little. In aggregate, however, we can create structured information that leads to deeper\\ninsights 2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 25}),\n",
       " Document(page_content=['When trying to quickly decide if something is a snake, we can rely on other cues. For instance, a\\nquick glance downward will probably tell us quickly what we are stepping on. In a sense, we would\\nlike to do the same thing with data in our enterprise', 'The challenge is that it can be difficult to give our data a “quick glance.” This could be because of\\nthe volume of data that we have, or the way the data is structured, or the fact that it lacks structure\\nand is, perhaps, scattered around in multiple locations. How can we take non-intuitive and perhaps\\nunrelated facts and assemble them into information?\\nSometimes data can be aggregated and transformed very easily. For example, ascertaining that the\\ntemperature of the hard drives in our storage arrays are at a safe level is easily done, provided we\\nhave proper instrumentation in our servers. No deep analysis is needed. But what if we wanted to\\npredict whether a particular drive is likely to fail in the next 72 hours?\\nOne of the strongest sets of tools that we have is the field of statistics. We can break statistics down\\ninto two general categories of tasks. Statistics is either descriptive orpredictive ', 'Descriptive statistics is about aggregating and transforming our data to produce new and interest-\\ning facts about our data as a whole or using these descriptive techniques to allow us to compare\\nsubsets of our data to the whole in a meaningful way. Predictive or inferential statistics allow us to\\nmake predictions about other data that we haven’t yet looked at or predict events that have not yet\\nhappened, based on the data that we have considered already', 'A related field of mathematics is probability theory. In fact, aspects of statistics rely heavily on\\nprobability. This field can be very useful for us in determining possible outcomes and likelihoods', 'It can also be very useful for classifying data (the act of classifying data produces new information)\\nand possibly identifying anomalies', 'Another set of tools that we have to transform our data are those provided by machine learning. No\\n2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 26}),\n",
       " Document(page_content=['doubt, these are the tools that you are most interested in if you are taking this course. We will dig\\ninto these beginning in book 3', 'A well-known mathematician, Hannah Fry, made the observation that machine learning, at least as it\\nis accomplished today, is possibly best described as a “…revolution in computational statistics…”\\nWe agree strongly with this observation. Indeed, this fact is the reason that we spend the entire\\nsecond book using statistics and other mathematics tools to manipulate our data before we ever\\nlook at what most would consider to be machine learning. Machine learning today can be broken\\ndown into two approaches, supervised learning andunsupervised learning\\nSupervised learning is used for classification and prediction', 'This approach is distinguished by the\\nfact that the training process involves providing the algorithm with both sample data and expected\\noutcomes . In fact, the primary distinction between supervised learning and unsupervised learning\\nis that, in unsupervised learning, we provide only data to the algorithm, no sample outcomes. This\\nleads to unsupervised learning solutions being used primarily for anomaly detection and clustering\\nproblems 2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 27}),\n",
       " Document(page_content=['The approaches and techniques that we used to acquire raw data and transform and assemble it\\ninto useful information is the field of data science. This is true whether the transformation that we\\nare performing is as simple as a statistical average or as complex as a many-layered convolutional\\nneural network. The tools used vary, but the goal is the same; transform data into useful information\\nthat we can then use to make decisions', 'We can break this down into a series of repeated steps that we will seek to master throughout this\\ncourse:\\n•Acquire data - Identifying and accessing data sources that already exist or possibly generating\\nmeaningful data', '•Preprocess data - Data, especially data that we acquire from some system or collection within\\nthe enterprise, is rarely in a form that lends itself to the analysis that we would like to perform', 'We must first preprocess the data, transforming it to a useful form and perhaps cleaning the\\ndata, removing outliers, filling in gaps, or otherwise converting the data to some standard', '•Organize the data - A hidden step in this process is data exploration. Remember that data, on\\nits own, is just a collection of facts. We will need to spend time between the previous step\\nand this step exploring the data to find interesting ways that the data can be organized that\\nresults in what appears to be meaningful information', '•Generate results - After the previous steps are complete, we can perform one final set of\\ntransformations, generating descriptions or inferences based on our data. These can then be\\npresented to stakeholders for decisions', 'Taken together, we can say that the purpose of these steps is ultimately to allow us to extract insights\\nfrom our data 2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 28}),\n",
       " Document(page_content=['It is generally agreed that the field of data science emerges from the intersection of other fields. All\\ndata scientists should have a background in mathematics and computer science, in addition to some\\nother specific knowledge domain', 'In a sense, we can view both statistics and computer science as sub-fields of mathematics. Even so,\\nmost would admit that we cannot expect a pure mathematician to write SQL code to interact with a\\ndatabase! Still, a mathematics background will be very helpful working in the field of data science', 'At a minimum, we must have some knowledge of statistics and how they are applied. However,\\nhaving at least a passing familiarity with calculus and linear algebra will help us to more easily\\nunderstand why or how statistics and other data science tools work', 'From the field of computer science, we need to have some level of competency in programming. At\\na minimum, we must be able to interact with our data storage systems and be able to successfully\\nextract data from them. We must also be able to organize and process that data programmatically,\\nso competency in at least one programming language is a necessity. While there are what amount\\nto “point and click” machine learning tools available commercially, if we are seeking to build some-\\nthing more complex or to apply these tools in more interesting ways, we must have competency in\\na language that interfaces directly with the API of the machine learning frameworks available', 'The last domain of knowledge that a data scientist needs is some field in which they have domain\\nexpertise 2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 29}),\n",
       " Document(page_content=['For this class, we will use the field of information security as our domain of expertise. What about\\nthe mathematics? Well, even if you have some mathematics in your background, for most working\\nprofessionals, this is likely many years in your past. We will therefore bootstrap our math and\\nstatistics knowledge over the course of our journey, reinforcing and reminding you of the most\\nimportant tools and concepts that will prove useful to you in your exploration of data. The computer\\nscience or programming domain will be satisfied with Python and the work we do learning to extract\\ndata from common data storage systems that you are likely to run into', 'Using these tools, then, we can follow the five-step process pictured on the slide. We (1) start\\nwith a question and develop a hypothesis about it. Next, (2) acquire data that might be useful in\\nproving or disproving your hypothesis about the data. To help us to figure this out, we (3) explore\\nthe data, visualizing it, running statistics on it, identifying correlations, and more. After doing this,\\nour data is likely ready for us to (4) apply some machine learning or other statistical techniques', 'This ultimately allows us to (5) share insights with our organization in some meaningful way 2'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 30}),\n",
       " Document(page_content=['Python\\nBased on the introduction, we are ready to embark on our data science journey. The remainder of\\nthis book and the associated exercises are intended to help you build or solidify your foundation in\\nthe computer science prerequisites that data scientists find useful', 'Please keep in mind what we said at the outset: There’s no way that we can hope to take you from\\nzero to data scientist in just one week. Therefore, don’t view the material covered in the following\\nsections as everything you need to know. Instead, view these as the minimal amount that you should\\nknow to be able to jump into documentation and wrestle your data science into submission 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 31}),\n",
       " Document(page_content=['One of the only prerequisites for this course is a basic knowledge of Python. If your Python back-\\nground is a bit weak, don’t fret. We’re going to spend a bit of time talking through the most important\\naspects of the language that you need right now, and you will have plenty of opportunity throughout\\nthe course of this class to put these skills into practice', 'On the other hand, if you have never used Python before, be prepared for a rough ride. Nearly\\neverything that we do in this class will make use of Python, and the expectation is that you met the\\nprerequisites before coming to the class', 'Finally, if you have lots of Python experience, please have some patience as we go through this\\nsection. We would also urge you to work through all of the exercises as well. Given your experience,\\nyou should find them easy to complete. Still, it’s worth putting in the time because, even with a\\nlot of experience, we tend to only master aspects of a language that we use, so there may be some\\nconcepts covered that are important but not a part of your normal use cases 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 32}),\n",
       " Document(page_content=['Lists and Dictionaries\\nJumping right in, let’s start with lists. As you no doubt know, within Python, everything is an object', 'Objects are instances of classes, which are generalized definitions of some abstract data type and\\nthe associated methods and attributes of that type', 'Lists are no different. Lists and the closely related array type within Python are a mutable list\\nof ordered values. In some other languages, you may find these using the underlying math term,\\n“Vector.”\\nLists and arrays within Python, like most languages, are zero-indexed. They are also iterables\\nwithin Python. This means that we can iterate, or sequentially traverse, the members of the array', 'There are other deeper meanings when it comes to iterables in Python, but they aren’t important for\\nus in the context of an array', 'Python arrays restrict you to using a single type for all elements. In many cases, this doesn’t seem\\nrestrictive, but when we are importing data from a variety of sources, this can represent a challenge The native list type does not have this same restriction', 'An alternative way of storing data is using a key-value store known as a dictionary. Dictionaries al-\\nlow us to use arbitrary keys as indices and assign any value to each. Dictionaries have no restrictions\\non the mixing of types in both keys and values 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 33}),\n",
       " Document(page_content=['When we have a collection of objects, whether they are in an array or a dictionary, one of the most\\ncommon activities is to traverse all of the nodes in that collection. In the code example above, you\\ncan see that the variable a_list has been assigned an array of values. The type, that is, an array\\nof integers, is inferred by the assignment', 'If you are somewhat less fluent in Python, it is important to know that the square brackets used are\\nspecific to arrays or lists. If we were to instead use parentheses, a_list = (1,2,3,4,5) ,\\nwe would be assigning a five-term immutable tuple to a_list , which is entirely different. Curly\\nbraces, similarly, have another meaning, which we will cover shortly', 'When working with an iterable class in Python, we can use the structure for item in col-\\nlection: to iterate sequentially over the values. This is true whether the iterable is an in-memory\\nstructure such as an array or some other structure like a cursor returned by a database library class', 'Of course, it is possible to use an approach that is common in most languages:\\nfor i in range(0,len(collection)):\\nprint(collection[i])\\nWhile this works as expected, this would not be considered to be a “Pythonic” approach. Pythonic\\nmeans that the code is not only syntactically correct and produces the expected output, but that\\nit also follows the conventions of the Python community and best fits the intentions of how the\\nlanguage is meant to be used. For our class, we aren’t particularly concerned about being Pythonic,\\nbut we will still try to write acceptably Pythonic code when we can', 'While either of the approaches above works just fine, they are not the most efficient ways to process\\nan array. We will consider these to be straightforward but naive approaches. This is not to say that\\nthese are “bad” ways to approach a problem, but they might not be the most efficient 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 34}),\n",
       " Document(page_content=['Before worrying about efficiency, we’d like to point out a very useful approach when traversing an\\narray. At times, we might be tempted to use the for i in range(0,len(collection)):\\nconstruction because we need not only the element in the array, but we also need its index for our\\nalgorithm. Even so, writing it in this way is not Pythonic. Again, while we are not fixated on\\nbeing Pythonic, there is a very useful construct that we can use in Python that simplifies our code\\nsomewhat:\\nfor index, item in enumerate(collection):\\nprint(f\"{index} : {item})\\nNotice that we are using the enumerate() function. This function returns a tuple with the index\\nof the current item and the item found at that index all in one step', 'Ultimately, it is up to you to write code that you understand and that performs the tasks that you\\nneed to be accomplished. However, you may find this construct very handy. This can be used with\\nany iterable class. Note, though, that this is no more efficient than the previous examples 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 35}),\n",
       " Document(page_content=['If you are interested in speed, there is definitely a better approach. Before going further, though,\\nwe need to bear in mind that while speed is good, readability is also important. This is especially\\ntrue if we want to be able to reuse our code easily', 'That said, the most efficient way to traverse a list or array is with a list comprehension . While there\\nis a separate map() function that takes a function and an iterable as arguments, a list comprehension\\nis effectively a mapping operation. This technique is both powerful and tricky', 'A list comprehension allows us to apply an operation to every element in the list, returning the\\nmodified list as its return value. This doesn’t mean that you must capture the returned value; for\\nexample, your comprehension might simply be used to print out each value. If you captured the\\noutput of the comprehension you would have a list of the return values from each of the calls to the\\nprint() function, which might not be particularly useful. On the other hand, what if our array\\ncontains a list of command strings that we want to execute using the oslibrary? We could use a\\ncomprehension to execute each string as a command and capture the return values, which would be\\nuseful', 'While all of the above is possible, the most compelling reason to use a list comprehension is ef-\\nficiency. You will find that the larger the number of elements in your list, the more positively a\\nlist comprehension compares with a for loop structure. The only tricky part can be figuring out\\nexactly how to write the transformation that you want to apply to each list element 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 36}),\n",
       " Document(page_content=['There is a far more computationally efficient mechanism that Python provides, though it can be a\\nbit difficult to parse when abused. The more efficient approach is the use of a list comprehension ', 'You may at first assume that a list comprehension is just a special shorthand that is effectively pro-\\nducing the same code as a for loop. This is nottrue. In fact, a list comprehension will typically\\noutperform the corresponding for loop for arrays of any significant size. Internally, a comprehen-\\nsion is a highly optimized operation', 'For a simple comprehension these are very straightforward to create and work with. Imagine our\\nfor loop looked like this:\\na_list = [1, 2, 3, 4, 5]\\nfor index, item in enumerate(a_list):\\na_list[i] = item * 5\\nThis will loop through our list, replacing each item with five times its value. Written as a list\\ncomprehension, we might use:\\na_list = [1, 2, 3, 4, 5]\\na_list = [i * 5 for i in a_list]\\nThis is definitely more Pythonic. For a simple operation like this, it is also very straightforward to\\nimplement and to read. Where we can get into trouble is when we start nesting these comprehen-\\nsions. We will look at an example soon', 'Notice, too, that an alternative mechanism to accomplish this same task would be to use the map()\\nfunction. This built-in function allows us to pass in a function to be called with each element of an\\narray as an argument. The function will automatically replace each element at each index with the\\nresult from the function called as the first argument to map()  3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 37}),\n",
       " Document(page_content=['If list comprehensions do what map() does, why do both exist? A simple answer is that arrays are\\ntypically held completely in memory. In fact, executing a list comprehension will operate on that\\nentire data set in memory. What if that data set has 10,000,000,000,000 elements? Even at one byte\\neach, this will require many terabytes of RAM!\\nThemap() alternative can be used with any iterable . An iterable is an abstraction to a collection\\nof data that allows us to access each element sequentially. An iterable, however, is not necessarily\\nan array', 'In fact, an iterable need not be in memory. The iterable could be an object that allows us\\nto iteratively access elements from a very large result set from a database query. (In this case, you\\nwill often run into the idea of a cursor in the database library documentation.) The iterable might\\nalso be another interesting Python feature, a generator . We will discuss generators much later in\\nour course since they can be very useful when creating training data sets. For now, suffice to say\\nthat a generator is called iteratively, returning subsequent values in response to each call 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 38}),\n",
       " Document(page_content=['Dictionaries, which we defined as key-value stores, also support comprehensions. Dictionary com-\\nprehensions can be useful, especially when exploring and building data transformations. This is\\nespecially true of data records from databases. These are typically exposed to use as rows of data\\nthat we must then extract or parse values out of', 'The structure of a dictionary comprehension is essentially the same as that of a list comprehension', 'The only big difference is that we must handle the key and value tuple handed to us in each iteration', 'You can see both the naive and the dictionary comprehension approach in the code example in the\\nslide 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 39}),\n",
       " Document(page_content=['Before we proceed, let’s be clear about our philosophy in the code examples you will see in the lab\\nexercises throughout this course. We believe it is far more important that code is comprehensible\\nthan it is efficient . This isn’t to say that efficient code isn’t important. However, we do believe that\\nit is far more important to get your code to work, however inefficiently, before you worry about\\nwhether or not it could be accomplished in a better way', 'Donald Knuth, a very well-known figure in the field of computer science, author of the seminal\\n“The Art of Computer Programming,” and creator of (among other things) T EX, the ancestor of the\\ntypesetting system used to produce this book, is known for having written and spoken extensively\\non his statement that, “premature optimization is the root of all evil.” What’s his point?\\nDr. Knuth was not saying that code should never be optimized. In fact, his work on formalizing\\nmathematical models for computational complexity is the primary way that we measure code ef-\\nficiency even down to today! What he was pointing out is that programmers will often spend a\\ntremendous amount of time optimizing code that has the least overall benefit to the efficiency of the\\nentire system. This is what we are trying to say', 'For example, while we might be able to create some incredibly efficient (though inscrutably arcane)\\ncode for preprocessing a dataset to then train our machine learning model, we may have spent a great\\ndeal of time on a problem that only impacts the initial ingestion of data during training. In the grand\\nscheme of how we use our machine learning solution, this code may be run only a handful of times!\\nInstead, if we wish to optimize something, we should identify the parts of the code that most greatly\\nimpact performance, otherwise known as the critical path ', 'With this understanding, you might think that this just sounds like common sense. Why do we have\\nthe tendency to optimize the wrong problems, then? The usual reason is that optimizing the critical\\npath is very difficult, whereas optimizing things outside of this path tends to be much simpler by\\ncomparison 3'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 40}),\n",
       " Document(page_content=['Consider this example:\\na = [1, 2, 3]\\nb = [10, 20, 30]\\nc = [3, 2, 1]\\nd = [x * y * z for y in a for z in b for x in c]\\nThis code is a trivial example of creating nested comprehensions. Can you predict what this code\\nactually does? Look at it and think about that question. Does it produce an array that has three\\nterms? Does it produce an array with six terms? Nine terms? 27 terms? We won’t answer this\\nquestion now, but we will see this example in our lab so that we can explore it', 'Rather than starting with a nested list or dictionary comprehension, we would recommend that you\\nwrite straightforward, naive code. Don’t worry if there’s a “better” or “more Pythonic” way to do\\nit. First, worry about making it work', 'Once you have code that works, only start worrying about efficiency when you have a piece of code\\nthat will be run millions or billions of times. Wait until it is either affecting performance or until you\\ncan foresee that it will definitely affect performance. That is the time to worry about optimization', 'Another way to think about this problem is this. If we were most concerned about writing super\\nefficient fast running code, would we be writing it in Python at all?\\n4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 41}),\n",
       " Document(page_content=['This isn’t to say that we can’t get highly performant solutions using Python. However, there are\\nthings that Python just isn’t great at. Since computational speed is one of those things, we need to\\nsupplement this weakness in some way', 'All modern machine learning solutions that have Python interfaces are implemented with NumPy\\narrays as their primary data structure. NumPy is an optimized numeric processing library for Python\\nwritten in highly optimized C code. NumPy extends the concept of Python arrays, allowing us to\\ncreate arrays of any number of dimensions and to mix types within each axis in an array. NumPy\\nalso supports some wonderfully useful ideas that are only possible through iteration with standard\\narrays, such as broadcasting  4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 42}),\n",
       " Document(page_content=['Let’s look at an example. Imagine that we have a Python two-dimensional array. The first axis,\\nwhich we could think of as “rows,” represent students. The second dimension, which we could\\nthink of as “columns,” represents grades on specific examinations. Let’s consider how we might\\ncalculate some averages of these scores', 'print([sum(row)/len(row) for row in grades])\\nThis first solution allows us to produce output that shows the average score of all exams for each\\nstudent. What if we wanted to produce an average test score for each test regardless of the student?\\nWe can certainly do this, but if you take a moment to think about it, you quickly realize that this\\nproblem is less straightforward. Could you solve this with a list comprehension of some sort?\\nDefinitely. Would it be better to write this out with for loops first? Probably', 'Let’s look at these same problems using NumPy:\\nnumpy_grades = np.array(grades)\\nprint(np.average(numpy_grades, axis=1)\\nprint(np.average(numpy_grades, axis=0)\\nThat’s it! What’s happening in here? First, we take advantage of the fact that we can pass any\\nPython array of any number of dimensions into the NumPy array constructor, and it will return a\\nNumPy array with that same data inside of it. Next, we take advantage of some simple functions\\nthat are built into the Numpy library', 'But how do these axes work? We’ll consider that on the next slide; first, a word about broadcasting', 'Broadcasting allows us to take a NumPy array and perform an operation against it with either a\\nsingle value or another array that might not be the same size, and NumPy makes that work. For\\nexample, if we take our array from above, numpy_grades , we could write:\\n4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 43}),\n",
       " Document(page_content=['numpy_grades = numpy_grades * 2\\nThis will take the entire array elementwise, regardless of the number of dimensions in the array,\\nand multiply each element by two. To accomplish this same task with Python using lists or arrays\\nrequires that we use the map() function, a list comprehension, or otherwise iterate over the entire\\narray. This is just one of the enormous benefits of using the NumPy array type 4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 44}),\n",
       " Document(page_content=['One of the things that people struggle with most when first working with NumPy arrays is the\\nnotion of the axis that a function is being applied to. The simple rule of thumb, which may not\\nseem intuitive, is that the axisisthe direction in which you wish to aggregate values', 'In our last example, we can choose to create an average for each student. Each student is found\\non axis 0, or each row. When we indicate that we want to apply our operation to axis=0 , we are\\nlooking across the rows. On the other hand, if we change this to be axis=1 , we are now summing\\nor averaging the columns within the data. We can extend this idea to any number of directions since\\nNumPy supports n-dimensional or arbitrary numbers of dimensions for arrays', 'Here’s a very simple way to work out which axis is which. If you are familiar with indexing arrays,\\nconsider the following. Imagine that we have a three dimensional array, which can be accessed as\\nfollows:\\nvalue_in_array = marray[0][10][20]\\nThis is perfectly normal code, and a code-form you have likely been exposed to in the past. It is\\nnatural for someone with some mathematics background to think of the indices as 𝑥,𝑦, and 𝑧. Don’t\\ndo this! Instead, use a different metaphor. If we instead think of this as 𝑟𝑜𝑤,𝑐𝑜𝑙𝑢𝑚𝑛 , and 𝑔𝑟𝑖𝑑 ,\\nwhere 𝑔𝑟𝑖𝑑 represents which matrix or grid of values we are referring to in this ℝ3space, axes\\nbecome very simple. Axis zero is the first dimension, or the row, axis one is the second dimension,\\nor the column, and so on', 'While you and I likely have great difficulty picturing more than three or four dimensions, we will\\nfind that the idea of using tens, hundreds, or even thousands of dimensions is not terribly unusual', 'However, we will also discover that there are different ways of describing the dimensionality of\\ndata 4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 45}),\n",
       " Document(page_content=['We don’t want to digress too far, but we do want you to give some thought to the idea of dimen-\\nsionality. People with a computer science or programming background tend to think of dimensions\\nin terms of indices into a multidimensional array. There is absolutely nothing incorrect about that,\\nbut you will discover that we can describe dimensions somewhat differently in data science', 'If we are working with a three-dimensional array in Python using the built-in list or array types, we\\nmight have a pattern like this:\\na = [1, 2, 3, 4, 5]\\nb = [2, 4, 6, 8]\\nc = [1, 3, 5, 7]\\nd = [a,b,c]\\ne = [d,d,d]\\nprint(e[0][1][2])\\nExamine the code above. Can you predict which value will be printed? The answer is 6. We ask\\nfor the first row (offset zero), the second array (which we can think of as columns), and the third\\nelement. We can think of this mentally as x, y, z coordinates (0, 1, 2) ', 'While this mental picture works, what if we had data with more dimensions? Most people reading\\nthat have a difficult time grasping what those dimensions must look like. Don’t bother! Forget that\\ncompletely because as creatures living in an observably three-dimensional world, we cannot visu-\\nalize what additional dimensions would look like. It turns out, however, that we have no problem\\nworking with multiple dimensions when we don’t realize it', 'For example, imagine that we conduct a survey. The survey collects a number of facts about partic-\\nipants, including:\\n4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 46}),\n",
       " Document(page_content=['•Income\\n•Age\\n•Years of education\\n•Ethnicity\\n•Religion\\n•Gender\\nIt would be reasonable to think of (Income, Age, Education, Ethnicity, Reli-\\ngion, Gender) as coordinates that represent a specific person. Indeed, if more than one person\\nhas the same coordinates, we could view them as equal from the point of view of the information\\nthat we have about them. You likely have no problem relating to this concept. Now stand back and\\nrealize that you have just easily conceived of a six-dimensional array or space without breaking a\\nsweat!\\n4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 47}),\n",
       " Document(page_content=['It’s now time for us to jump in and work on a lab. As is true with anything, talking about the task\\nwill rarely teach us how to do it. Instead, working through solving a series of problems will allow\\nus to build fluency', 'The first lab also includes a bit of a “setup” section. You will first work through getting your VM\\nset up, Anaconda installed, and all of your workbooks unzipped. Once these tasks are completed,\\nyou’ll turn your attention to a series of challenges that we will ask you to solve using Python. Don’t\\nworry if your Python is a bit fuzzy; the labs walk you through ways of thinking about the problems\\nand reasoning out solutions. Also, don’t forget that there is a set of solution notebooks that has all\\nof the exercises completed', 'If you are working through this class in a self-study or OnDemand modality, please bear in mind that\\nthe lab exercises always cover what we’ve just covered in a section, but they also go well beyond\\nwhat was in the slides or the notes. If you are truly looking to master this subject, this book is not\\nenough; you must also work through the exercises yourself! Don’t assume that just looking at the\\nsolutions in the solution notebooks is sufficient to truly understand what’s happening. It is very true\\nthat we tend to learn more by trying, failing, and fixing 4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 48}),\n",
       " Document(page_content=['Lab 1: Python\\nPlease use your workbook to complete Lab 1!\\n4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 49}),\n",
       " Document(page_content=['Matplotlib and Plotting Basics\\nIn this section, we’re going to take a look at how we can go about taking our data and turning it into\\nuseful visualizations. In order to do this, we will be looking at a commonly used Python library\\ncalled Matplotlib. This free, open-source solution is widely used in its primary form, but you will\\nalso find that it also exists as the main library leveraged by many other graphing and visualization\\nsolutions available for Python', 'While there is far more to this library than we will cover here, we do want to cover a few of the\\nbasics before we get you to work hands-on with it in our lab. We will also expand on some of the\\ntopics that are covered here, in addition to examining more features as we make use of this and\\nother graphing capabilities throughout the labs in this course 4'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 50}),\n",
       " Document(page_content=['The reason that we need a tool like this one is that we will frequently want to be able to visualize\\nour data. This is a very important aspect of the exploration phase, which we will examine in-depth\\nin book two. The exploration phase is focused on getting to know what’s in our data and trying to\\nidentify relationships in addition to interesting facts about our data that we can leverage throughout\\nthe data analysis process. This allows us to transform data into a visual representation. This done,\\nwe can look at one or more fields or perhaps examine multiple fields in comparison to one another', 'We might even perform correlation analysis and create a visual representation of how closely tied\\ntogether, or perhaps even unrelated, fields are', 'It is also not unusual to make use of visualizations as part of the output phase of whatever machine\\nlearning or a visualization/analysis process we’re trying to create. Even if the output of our system\\nwill not be visual, we will still be able to leverage visualizations during the training process to\\nmeasure how well our models are performing. We will look at this as we work through TensorFlow\\nand work with TensorBoard to visualize The performance of our models as we are training them TensorBoard uses matplotlib under the hood 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 51}),\n",
       " Document(page_content=['A tremendous advantage to using this library is that since it is used in almost every other graphing\\nlibrary as the underlying toolset, we will be in a very good position to make use of almost any other\\nvisualization tool available in Python. Since the stool is still actively developed, unlike some other\\nalternative solutions, we can also have some confidence that we will be able to use this tool not only\\ntoday but for some time into the future', 'Even though this is a free library, it is still full-featured. It supports almost any kind of graph that\\nyou can imagine. Certainly, there are libraries that build on top of matplotlib that create much more\\nattractive looking plots, but from our point of view, we are much more interested in how useful the\\ngraph is than we are in how pretty it might look', 'There is one thing to be mindful of when using this tool for visualizations. The more points you\\nattempt to graph, the slower it will be. Anecdotally, our experience is that the labels that are added\\nto the data, even if they are only the labels for the tick marks on the axes, can dramatically impact\\nperformance. You may want to bear this in mind if you are trying to visualize a large set of data and\\nit is taking a very long time to complete 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 52}),\n",
       " Document(page_content=['When using this library, we will usually only import one main portion of it. This is the pyplot section\\nof the library, and by convention, we typically import it with the mnemonic plt. Once we have\\nthis imported, there are just a few basics required to get a graph upon our display. Of course, we\\nneed to have some data to graph, so in the slide, we are creating a set of random values', 'The function that we are using to create random values comes out of NumPy. The np.random()\\ncall allows us to specify any number of random values that we would like NumPy to produce. The\\nrandom values will be returned in a NumPy array. In this case, we have added the normal method\\nto this call, which allows us to specify that we would like the values to have a normal distribution', 'We will talk a bit more about what a normal distribution is in our statistics section. This function\\ntakes three parameters. The first is the average value around which we would like our random value\\ncentered. The second is the width of the standard deviation, which we will define tomorrow. The\\nlast is the number of values that we would like to produce', 'Having created the data, we can simply call the plt.plot() function, passing in the array as the\\ndata to be plotted. While this would be sufficient within a Jupyter notebook if the creation of the\\nplot is the last item in the cell, we really should be in the habit of calling the plt.show() method\\nto cause it to be displayed. This is especially important if we’re creating multiple graphs. We can\\nmake adjustments to this plot all the way up until the time the show method is called. However,\\nif we were to create this graph and then immediately create another without ever calling the show\\nmethod, our first graph would be lost 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 53}),\n",
       " Document(page_content=['We will frequently want to create multiple graphs out of a set of data. Perhaps our data has multiple\\ncolumns, and we would like to create a graph for each column so that we can compare one to another', 'Another possibility is that we would like to create different graphs of the same data, with each plot\\nvisualizing it in a different way. We may also be dealing with a situation where we would like to\\nrender a number of different images, perhaps in a grid. All of these things can be accomplished\\nusing subplots', 'When we call the plt.subplots() method from the library, it will return a tuple containing the\\nfigure object in which all of the subplots plots are held and an array containing all of the subplots\\nrequested. The plt.subplots() method call takes a single argument. This argument can be\\na single number to indicate how many plots there should be. You can also pass a tuple indicating\\nhow many rows and columns of subplots should be created', 'Once we have created the subplots, we can utilize each one of them to plot data using any of the\\ndifferent visualization methods. In the case we have in the slide, we are looking at the same data in\\ntwo different ways. We are not limited to this. We might look at different pieces of data in different\\nsubplots. We could also choose to use some of the subplots for graphs and others for other types\\nof visualizations. Each subplot can contain any rendering that the matplotlib library supports. This\\nwould include images, text, or other elements 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 54}),\n",
       " Document(page_content=['Generating subplots is a very common task. There are different strategies for handling these sub-\\nplots, but one of the easiest is to make use of the flat attribute and then enumerate the array in a\\nloop or comprehension. Please take note that this is an attribute and not a method! While some doc-\\numentation says that this creates a copy of the array, it is much more accurate to say that it creates\\nan iterable reference to the array. This iterable reference allows us to view the array as a flattened,\\none-dimensional structure', 'Since the enumeration isn’t creating a new copy but instead is giving us references to the individ-\\nual subplots, this provides a very easy way to sequentially populate each plot without having to\\nkeep track of which row or column we’re in. As you can see in the example on the slide, we can\\nsimply refer to each of the subplots using the index from the enumeration. Also, take note that the\\nplt.show() method is only called after all subplots have been rendered. Calling it before this\\nwill prevent the following subplots from being displayed 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 55}),\n",
       " Document(page_content=['This library has a tremendous number of features. It is also, unfortunately, one of the least intuitive\\nlibraries that we will make use of in this class. Given how important visualization is, however, it is\\ndefinitely worth our time to work through some exercises to make sure that we understand it well\\nenough to be able to use it with some facility rather than having to look up the manual pages every\\nsingle time we want to create a plot', 'One of the problems that we will solve in the lab is what to do when our subplots begin to become\\ncrowded. As this happens, the text on the axes or perhaps in the titling will become unreadable. We\\nlook at some easy ways to solve this in addition to some ways that we may choose to customize this\\nto move legends and titles inside of the graph. We will also look at how to go about adjusting the\\noverall size of the graph, change the type of lines that are used in rendering the graphs, how to adjust\\ncolors, and several other features. There truly are many different tricks to using this library. Even\\nafter the introduction that we cover in our lab today, you will continue to learn about additional\\nfeatures in this library for the next several days 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 56}),\n",
       " Document(page_content=['Exercise: Matplotlib\\nAs with all of our labs, this lab does build on the last lab. If you have not yet completed the first lab,\\nyou should go back and do this now. This lab will expect that you fully understand and can use all\\nof the language features that were discussed and used in the first lab. This includes using Python\\nfeatures that we practiced in addition to being familiar with the NumPy library features covered in\\nthe first lab. When you are ready, please open the next workbook in Jupyter Lab 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 57}),\n",
       " Document(page_content=['SQL Crash Course\\nIn this section, we will try to give you a fast and practical introduction to the SQL language and\\nrelational databases in general. This language and the technologies that make use of it are very\\ncomplex. Much like Python, there is no way that in just a few minutes, we can tell you about every\\nfeature and option. However, our hope is that coupling the discussion in the slides, the notes, the\\ninstructor discussion, and the hands-on exercises that follow, you will gain a solid familiarity with\\nthe concepts and techniques', 'After you complete the labs, you should be able to connect to, interact with, query, and extract\\nresults from a SQL database 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 58}),\n",
       " Document(page_content=['The SQL language has been around for many years. It represented a revolution that changed how\\ndata was stored and structured. Most databases prior to the creation of SQL were based on ISAM,\\nVSAM, or other proprietary database technologies. Both of these technologies still exist today (and\\nare used!), but SQL has dominated this space for decades. Even in cases where ISAM tables are\\nused to store the data, SQL is most frequently the interface that is used to access this data', 'The SQL language is a DSL1that can be broken into four main pieces. One part, the DDL (Data\\nDefinition Language), is used to describe the structure or schema of the database. This includes the\\ncreation of the data tables and the various fields, often called columns, that are found within each of\\nthose tables. Another, the DML (Data Manipulation Language), is used for making changes to rows\\nwithin database tables', 'This includes inserting, updating, deleting, and other operations that cause\\nchanges to occur to the data within the fields. The third is the DCL (Data Control Language)2. This\\nis used for defining data access and authorization requirements and restrictions that are applied to\\ndatabases, tables, rows, or columns. While these three are quite important, we are not particularly\\ninterested in them if our only goal is to be able to extract data from the database for data analysis\\nor machine learning purposes. Certainly, if you have the time, we recommend that you spend time\\nlearning about the DDL dialect used by database products within your enterprise so that you can\\nwell understand the different data types available in those servers', 'The aspect of SQL that we care a lot about is the DQL(Data Query Language). This is the part of\\nthe language that allows us to request data out of the database. This language can be quite complex\\nsince we are dealing with relational data. This means that the data in one table can be used to access\\ndata from another. We will get into that in just a little while', '1Domain Specific Language\\n2In the event you choose to research this, you should be aware that there have been several other computer languages\\nthat use the abbreviation “DCL.”\\n5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 59}),\n",
       " Document(page_content=['Even though the language that we use to access the database can be very complex, or at least can be\\nused to create very complex queries, the ideas behind a relational database are not very complicated\\nat all. The mental picture that you can use is that of a set of Excel worksheets. Each sheet has rows\\nand columns, where each row has information about one specific record. The columns within that\\nsheet are used to store facts about each one of those rows or records', 'What we have so far would be called a table. When we take multiple tables and put them together\\n(think of an Excel workbook) we now have a database. Therefore, a database can be defined as a\\ncollection of tables, each of which contains rows made up of columns or fields that contain infor-\\nmation. Even though this is a database, it can’t be described as a relational database yet. For it to\\nbe a relational database, we would expect that some of the data in the various tables are related to\\nor connected to rows in other tables', 'On the right-hand side of our slide, we have a piece of what is known as an entity-relationship\\ndiagram, or ERD. These diagrams depict the relationships between the different tables and show\\nthe structure of the data within the tables. In this particular diagram, the “Customer Data” database\\nhas four different tables within it. These tables each have various pieces of information or fields that\\nare somehow connected to the customer. You can see that there are lines connecting these tables to\\none another', 'There are also lines that run off of our diagram that connect these tables with tables\\nin other databases. These lines indicate the relations that exist. For example, if you look at the\\naddress table, you will see that there is a field called city_id . This field represents what is called\\naforeign key from the city table. In other words, when we store an address, we don’t actually store\\nthe city related to it. Instead, we only store the city_id of the corresponding city record where\\nthis address is located', 'These relationships can become very complex, and there are both benefits and disadvantages to\\nusing SQL databases 5'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 60}),\n",
       " Document(page_content=['The Data Query Language\\nThe DQL can be very simple and straightforward to use. One of the design goals for the original\\nversion of what has become SQL was to create a query language that expressed itself in something\\nclose the English. When we look at the most basic queries, this is certainly true', 'After connecting to the database server, we must tell the server which database we wish to work\\nwith. When working interactively, we do this using the use <database> command. Once this\\nhas been done, all of the tables within that database can be easily accessed. It is possible to query\\ntables in other databases as well, but this isn’t a common use case for us', 'Once the database has been selected, we can now request data. To do so, we use the SELECT\\nstatement. These days the case of SQL statements is not particularly important, but traditionally the\\nSQL language portions of our statements are capitalized. Many DBAs (database administrators)\\nand programmers prefer this convention since it makes the SQL language portions stand out from\\nthe tables, fields, and data. We will not be pedantic about this in our class', 'The general format of\\ntheSELECT statement is:\\nSELECT <* | column[, column, ...]> FROM <table> [WHERE <condition>]\\nHere we have used the standard convention of indicating required arguments using the <>and\\noptional arguments using []. In aSELECT command, we specify which columns we are interested\\nin seeing out of the rows that are selected. If you aren’t sure what the names of the columns or fields\\nare, you can ask for *, which returns all fields for the selected rows. We must also specify which\\ntable we are interested in querying with the FROM clause. The WHERE clause is optional; it allows\\nus to limit which rows are returned using some selection criteria', 'These basic select statements are very straightforward and easy to grasp. Let’s make them slightly\\n6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 61}),\n",
       " Document(page_content=['more complex. One of the strengths of using an RDBMS is the fact that it is a relational database', 'It is, therefore, very common to link tables together in queries. Here’s a basic example of using a\\nrelation:\\nSELECT users.name, orders.number, orders.total\\nFROM orders\\nINNER JOIN users\\nON users.id=orders.userid\\nThis looks far more complicated, but there’s really not much more happening. We are asking to see\\nthename column from the users table in addition to the number andtotal columns from the\\norders table. Wow! This means that we are extracting columns from two tables at the same time', 'How does it know which records to show us from the orders table with the data from the users\\ntable? Notice the INNER JOIN clause. This expresses that we want to see orders rows for each\\nuser where the users.id value is the same as the orders.userid value 6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 62}),\n",
       " Document(page_content=['From our perspective as budding data scientists, we are fortunate that we need not overly concern\\nourselves with the design of the database or tables. This isn’t to say that we never care, however. For\\nexample, we might be working on an ETL (Export, Transform, Load) task where we are attempting\\nto query multiple tables in multiple databases simultaneously in order to aggregate and transform\\nthat data in an important and useful way for our enterprise. Once that transformation has been done,\\nwhether it is a simple aggregation, analysis, or MapReduce style operation, the results might be\\nloaded into some other data store (possibly even a SQL database). In a case such as this, we would\\nneed a great deal of knowledge of the structure of the data tables queried', 'An important mental key to working with SQL is to view the operations as manipulations of sets\\nof data. If we use this mental picture rather than viewing things as rows and columns, the various\\noperations tend to make a bit more sense. The queries themselves can end up looking incredibly\\ncomplex, but ultimately we are just mixing, matching, and manipulating sets of data 6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 63}),\n",
       " Document(page_content=['Just to give you a concrete example of what a somewhat more complex query might look like,\\nconsider the example in the slide. This SQL query is a mixture of Ruby code and SQL. The database\\nis used to store log events from Windows event logs and various system logs from routers, switches,\\nand UNIX systems. One of the criteria for the system design was that it be very responsive when\\nit comes to arbitrary text searches across events. The developers took an interesting approach to\\nsolve this problem', 'When an event is stored, it is first broken into discrete words. The system evaluates each word,\\nchecking to see if that word has ever been seen before. If it has, it retrieves the unique ID for that\\nword. If it hasn’t, the word is inserted into the words table, and the ID is captured. Now that all\\nof the IDs are available, the system stores rows into the events_words table, which links the\\nevents tables to the words table, storing both the event_id andword_id for every word\\nfound in an event', 'Later, when someone wants to query these tables, the code in the slide is executed. This takes the\\nlist of search terms that are passed in by the user, expands them out to create multiple LIKE clauses\\nin a nested SQL query, and then finds all events that have all of those words present. The query\\nitself looks complicated since there are so many pieces happening. Indeed, this could have been\\naccomplished in multiple discrete steps, first retrieving and storing the words, then retrieving the\\nevents, then performing the aggregation. While this would be more tractable to read, the perfor-\\nmance would suffer greatly. By writing it as a set of nested queries, the SQL server can do this in a\\nhighly optimized way, simply generating intersections of the relative sets and returning final results\\nrather than creating and returning three different sets of results', 'When working with SQL databases, you will frequently hear the term “Normalized” or “First Nor-\\nmal Form” or some other “Normal Form.” What do these mean? A normal form defines a standard\\nway of representing data that seeks to minimize duplication. How much duplication has been elim-\\n6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 64}),\n",
       " Document(page_content=['inated and how highly optimized the data storage depends on how far we go with normalization', 'In common use, you will find first normal form (1NF) and second normal form (2NF) databases', 'This isn’t to say that they can’t be third or more, but they are definitely more rare to encounter. The\\nentire idea behind normalization is eliminating duplication in the database and speeding access and\\nuniqueness of indices. That’s it', 'First normal form means that every column in a table contains only a single value (rather than a\\ncomposite value) and that there is some field within the aggregate rows that is row-wise unique in\\nthe table. This unique field serves as an index', 'Second normal form extends this such that there is no duplication across table joins. In other words,\\neverything in the right side of the join is unique to everything on the left side of the join. This results\\nin faster access since the result sets should be commensurately smaller, or at least more specific to\\nwhat you are retrieving', 'There is no value for us in exploring the other normal forms since you are less likely to encounter\\nthem and since we are not trying to teach an RDBMS class. Just take away from this that normaliza-\\ntion is about reducing the number of times we store the same data, preferably making data unique', 'While higher levels of normalization are often better, they are not always better. We are trading\\noff data storage (we need less space to store more highly normalized data) for access time (it will\\ntypically take more time to reconstruct data that has been highly normalized) 6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 65}),\n",
       " Document(page_content=['SQL Joins\\nThere are many types of joins or ways of relating tables to one another. The list in our slide is by\\nno means complete. In fact, you will find that there are different ways of expressing the same join', 'What some would call a Difference join, others would call an Outer join. Knowing this at the outset\\nis useful. We want to provide you with some fairly standard vocabulary, but don’t get too hung\\nup on the precise names for things since different vendors and different experts may use slightly\\ndifferent terminology. In the context of this class, we will define the joins as follows:\\nJoin Meaning\\nInner Select rows from the table on the “left” (table A), returning the column values\\nfrom rows in both the left and right tables where the fields in table B are related\\nthrough a foreign key in A', 'Left Perform an inner join where possible, but if there are rows in table A with no\\nmatches in table B, return the table A rows anyway with the columns from B set\\nto null', 'Outer Also called a Full Outer Join and sometimes a Union. Return all of the rows in A,\\nwhether there are relations in B, all of the rows in A where there are relations in\\nB, and all of the rows in B where there were no relations in A. In other words,\\nreturn everything ', 'Cross Not all database servers support a cross join. The idea is to return all\\npermutations of the rows in column A with the related rows in table B', 'Right The opposite of a Left join. Return all of the rows from table B where there are\\nrelations in table A 6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 66}),\n",
       " Document(page_content=['Join Meaning\\nDifference Also called an Outer join by some. Return all of the rows in the two tables where\\nthere are norelations between them 6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 67}),\n",
       " Document(page_content=['We have attempted to picture the most common joins in the images in the slide. It is often easier to\\nunderstand what is happening in the joins by visualizing what they mean. As with our definitions\\nof join types on the previous page, do not assume that these are all of the possible joins. There are\\nmany more ways that we can mix and match our tables, and there are many other words that you\\nmight find an administrator or documentation using to describe a particular join. In our experience,\\nthe best way to disambiguate how the data is being retrieved is to force someone to draw a diagram\\nto illustrate it 6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 68}),\n",
       " Document(page_content=['In addition to being able to join or relate data in one or more tables simultaneously, most SQL server\\nimplementations support a variety of aggregation and grouping functions. Aggregation means that\\nwe are collecting data and summarizing it in some way. Grouping means that we are putting all\\nrelated records together in one section', 'The aggregation options largely revolve around simple statistics operations that we might want to\\nperform on our data. This can be very useful when our data has been stored properly, allowing us\\nto perform calculations very rapidly across vast numbers of rows. Unfortunately, sometimes we\\nfind that the data is not stored using ideal typing, which might mean that these operations cannot be\\nused. For example, if we’ve stored currency values in a string field with a currency indicator, we\\ncannot easily use the AVG,SUM,MAX,MIN, or other operators on that data. We would first have to\\ntransform the data, which is significantly more expensive', 'The worst case is always to extract the data from the database and then perform our aggregation', 'Just because it’s the worst-case doesn’t mean that we won’t do it. We will talk about a variety of\\nstatistical operations in the following days, many of which are extremely useful but completely\\nunavailable in the SQL language 6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 69}),\n",
       " Document(page_content=['We have only scratched the surface when it comes to SQL and the various complexities of writing\\nqueries. Since we are only focused on practical applications from a data science perspective, we\\nare solely interested in getting you enough familiarity to be able to run basic queries. With that\\nknowledge, you will be able to extract data from databases on your own. You should also be in a\\nmuch better position to know what to type in as a set of search terms to your favorite search engine\\nwhen you need to go further. At a minimum, you should be in a better position to look up the\\ndocumentation for the specific SQL dialect supported by your enterprise’s SQL server', 'When interacting with SQL servers, don’t be shy about talking to your DBAs as well. A data\\nscientist has broad knowledge covering programming and data access languages. A DBA has very\\nspecific and deep expertise in administering SQL servers and optimizing queries. Leverage the\\nexpertise that these people have! We are often surprised at the counterintuitive recommendations\\na DBA will make regarding our own SQL queries. Their experience has taught them how to take\\nadvantage of (or sometimes trick) the optimizer built into the SQL server to result in the most\\nefficient queries 6'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 70}),\n",
       " Document(page_content=['It’s now time for us to put this into practice. Please switch back into Jupyter Lab and get started on\\nthe SQL notebook. In this lab, you will have the opportunity to connect to and run queries against a\\nPostgreSQL server running on the course VM. We have a prepopulated set of data tables there that\\nyou will explore, attempt to gather some statistics on, and experiment with running joins of varying\\ndegrees of complexity 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 71}),\n",
       " Document(page_content=['NoSQL Document Stores\\nAs powerful as SQL is, it isn’t always the best way to go. While SQL databases can allow us to\\ndeduplicate data reducing in more efficient usage of space and can also allow us to create powerful\\njoins of data across multiple tables or even databases, another approach sacrifices space for even\\nmore efficient retrieval performance. This is the general motivation behind the NoSQL paradigm\\nand document stores 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 72}),\n",
       " Document(page_content=['These types of databases come in two main flavors. The first is a document store, of which Mon-\\ngoDB is an example. The second is a key-value store, an example of which is CouchDB. All\\ndatabases in these categories fall into the “NoSQL” class', 'In a document store, the documents themselves can have any structure we would like. The data\\nwithin the document store is organized into collections . We can think of a collection as a table in\\na SQL database. The visibility into the contents of the document within a collection is one of the\\nreal strengths of a document store', 'Another strength in this type of database is when we retrieve\\na document, all of the data associated with the document comes along. While this can lead to\\nsignificant data duplication and greater disk utilization, it also means that the retrieval of data is\\nvery fast and can far exceed the speed of a SQL database. Why is this? Because we don’t need to\\naccess multiple tables or databases to retrieve the relevant record and all of the related fields from\\nother tables. The other advantage of using a document store over a key-value store is that we can\\nuse elements or fields within the documents in our searches, rather than being limited to only using\\nthe keys. In other words, this would be like having a SQL database where the only way you can\\nlook up any record in any table is with the ID field', 'A key-value store is very similar, except the contents of the documents are completely opaque. Since\\nwe cannot see or query based on the content of the documents, it makes no difference whether the\\ndocuments have an internal structure or if they are just binary blobs of data. Queries, as a result, are\\nvery highly optimized. However, it can also be very limiting since there is no way to do a relation of\\nany kind. In this class, we will use document stores rather than key-value stores. In our experience,\\ndocument stores are more common to find in enterprises today, while key-value stores are more\\npurpose-built systems used in more of a niche role 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 73}),\n",
       " Document(page_content=['MongoDB\\nIn our class, we will make use of the MongoDB database as an example of a document store. We\\nhave chosen to use Mongo with this class since it is both simple to use and is a relatively lightweight\\ndatabase solution. In a variety of our exercises throughout the course, we may want to temporarily\\nstore data into some kind of data store rather than a simple flat file. In these cases, we will often\\nuse Mongo as this temporary storage medium', 'While SQL databases could certainly be used as well, creating a SQL database requires more upfront\\nwork and can be more challenging. The structure of the entire database must be defined upfront', 'Using a document store, we can define the structure of elements that we care about and simply\\nput elements that are less important into unstructured data fields for analysis at some future time', 'Additionally, SQL databases can be somewhat heavy in terms of system resources, especially if\\nwe are using a laptop or workstation for our data research. Of course, if you are interacting with\\nyour enterprise data store, you must use whichever language it exposes to you. These days, large\\nenterprises usually use both traditional SQL databases and newer document stores; which storage\\nsolution is used is determined by the specific use case of the application. However, our experience\\nis that medium to small enterprises that have existed for more than 10 years will tend to use SQL\\nservers only, while very new tech startups will tend toward document stores', 'When working with Mongo and most other document stores, the primary mechanism for defining\\nobjects is JSON. JSON, or JavaScript Object Notation, isn’t difficult to understand, but it can take\\nsome getting used to if you haven’t worked with it before. The more difficult aspect of working\\nwith JSON in document stores is that all of our queries are also structured as JSON objects that\\nare passed to the document store. This can be quite challenging when you were first beginning to\\ninteract with this type of database, especially when you come from a SQL background 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 74}),\n",
       " Document(page_content=['To make our lives easier, we will make use of the MongoEngine Python library. This leverages a\\nlower-level library called PyMongo. In our lab, we will look at both MongoEngine and PyMongo', 'While our preference would be to use MongoEngine only, there are some tasks that we will need\\nto do when exploring data that we will require either a Mongo command line or a PyMongo script', 'Since we are most interested in generalizing our work so that it is repeatable, we will take the time\\nto learn a few elements of the PyMongo library to make this possible 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 75}),\n",
       " Document(page_content=['It might surprise you to hear that large enterprises and small have moved to document stores over\\ntraditional databases. There are good reasons for this shift. We should acknowledge, though, that\\nsome organizations make use of document store technologies without adequately understanding the\\nstrengths and weaknesses of this technology', 'One of the greatest strengths of a document store over a traditional SQL database is that the schema\\nof the database can be changed very quickly and easily. In an SQL environment, changing the\\nschema requires that every existing record in the database is updated to match the new design. If you\\nhave ever worked through performing a database migration1, then you appreciate how fraught with\\nperil such an update is. What happens when a particular record is structured slightly differently than\\nwhat is expected? Will the adjustment be made properly during the migration? Will the migration\\nfail halfway through? What happens to existing applications that expect the design to have a specific\\nstructure and that structure has changed? All of these things make changing the schema a very\\nchallenging process in a traditional database', 'Changing the design of a document using Mongo is trivial. Do you have a new field that you would\\nlike to add? Just add it. There is no need to go back and add it to every existing record unless you\\nwant to. Would you like to remove the field? Then remove it! You don’t actually have to update\\nthe data to make such a change. Of course, you can run into trouble if you change data types. The\\ndatabase may not care very much, but your applications definitely will', 'Since the design of the data is so flexible, it is very easy to go quickly from an idea to code that\\nis storing data. With a traditional database, we must spend a great deal of time thinking about the\\ndesign of the data. Since a document store allows us to restructure the data on the fly, much less\\nupfront design time is required. We should mention that while this is a strength, this can also be a\\n1Database migrations are the copying and updating of the content of database tables when the underlying schema\\nis changed 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 76}),\n",
       " Document(page_content=['great weakness. This can certainly lead to developers simply writing code without doing anydesign In no way are we suggesting that that is a good idea', 'One of the greatest strengths to operating a document store, and the main reason that an enterprise\\nwould choose to do so, is the retrieval speed. While we can create relationships between documents\\nwithin a collection or between collections, the use case for which the systems are designed is that\\nall of the data related to a record is stored together as a single document. This means that we are\\ntrading off the size of the data being stored (relatively large in a document store) with the speed of\\nretrieval (very fast in a document store) 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 77}),\n",
       " Document(page_content=['While these benefits are wonderful, there are some potential deficiencies. While our course is not\\nfocused on securing things, since our domain expertise is in the field of information security, it\\nis notable that vulnerable document store installations are rampant. It takes minutes or possibly\\neven seconds to stand up a new document store. You can immediately begin to interact with it', 'Unfortunately, the default deployment will be terribly insecure if remote connections are enabled', 'While we are not aware of any studies to back this up, our experience is that the state of security of\\nthe deployment of document stores is far worse than that of SQL databases. While SQL databases\\ncan also be configured insecurely, since they are more complex systems and won’t “just work,”\\nadministrators tend to spend more time making sure the configuration is correct', 'There are other aspects of using a document store that are not so good. For example, if your use case\\nrequires that data is retrieved using anything other than the default _id field within the collection,\\nthe performance of the retrieval will not be optimal. It is possible to create additional indices for\\nother fields within a document or even for subdocuments with the documents in a collection. If this\\nis being done frequently in a project, it calls into question the decision to use a document store over\\na SQL database', 'Since we are now adding additional indices and are likely writing queries that look\\nvery much like SQL joins, we are losing all of the speed and performance benefits that a document\\nstore provides. In other words, if we are using the document store like a SQL database, why aren’t\\nwe just using SQL?\\nAnother potential downside, which might not matter that much to you, is that the high retrieval\\nspeed comes at the sacrifice of space. When creating a document store, we do not worry about\\nnormalization at all. This means that you will likely have a great deal of duplication within the\\ndatabase. This can feel really wrong for people who come from an SQL background', 'In fact, while we will likely have some degree of duplication, you can probably sense that a signifi-\\ncant amount of time should be spent in planning how the data will be used in an effort to prevent this 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 78}),\n",
       " Document(page_content=['We want to be sure that our documents are being stored in a way that minimizes duplication. If we\\nfail to do this, then changing or deleting a record will either require that many records be examined\\nto be sure that they do not need to be updated, or our data will become inconsistent depending on\\nwhich document is examined. This can mean that your initial intuition about the structure of your\\ndocuments might turn out to be exactly wrong. If good planning isn’t done, this can result in either\\nan application that performs poorly or it will require a great deal of rewriting when we later perceive\\nthat the document structure isn’t ideal 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 79}),\n",
       " Document(page_content=['To illustrate these ideas, think about how we might store Data related to Twitter users, followers,\\nand tweets. At first glance, our intuition might be to store a user document that contains the user ID,\\npassword, screen name, tweets, followers, and followed. When someone queries our web interface\\nto view the tweets of a user, a single database query is required that returns a single document\\nholding all of the data that we need. Obviously, some of that data is not displayed, for example, the\\npassword. However, the entire tweet history is present. We can also tell how many followers there\\nare and how many people this user is following', 'Think about what would have to happen if the account for an arbitrary user’s followers were deleted', 'Naïvely, we could simply delete that user as a document out of the collection. But now, what\\nhappens when we view a user who was connected to that deleted user? Is that user still a follower?\\nNo. They have been deleted. When do we update the follower count? Should we follow every\\nreference for followers and followed when the deletion is processed and update every one of those\\ndocuments? That sounds very expensive', 'Perhaps we should only check to see if the followers and followed still exist when any specific user\\nrecord is retrieved. This does make the initial deletion far less expensive, but we must now attempt\\nto retrieve the user document for every follower and every followed user to check to see if any have\\nbeen removed! This is also very expensive and is likely far more expensive since we need to do\\nthis retrieval every time the user document is retrieved', 'Since our class isn’t about good document design in document stores, we are not going to try to\\nsolve this problem. Our focus is the ability to interact with and query document stores 7'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 80}),\n",
       " Document(page_content=['Interacting with Mongo using the command-line utility is similar to interacting with a SQL database,\\nnot quite the same. If you have previous experience with SQL, then you will see things that feel\\nvery familiar. However, you will also find that there are some commands that are just gone', 'Much like using a SQL database, we must first choose which database we would like to interact\\nwith. To determine which databases or document stores are available, we can use the command\\nthatshow dbs . As you can see in the slide above, this will give us a list of the databases, or\\ndocument stores, that are available along with the total utilized space for each. To choose to use\\none of these, we use the same command that would be used within SQL: use followed by the name\\nof the database that we wish to query', 'Similar to an SQL database being made up of tables, the Mongo database is made up of collections', 'To see which collections are available, we can use the show collections command. So far,\\nso good! It might seem that everything that we learned with SQL can be applied here directly, but\\nthat is about to change 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 81}),\n",
       " Document(page_content=['To query one of the collections, or to do anything else for that matter, we use an object-oriented\\nconvention to both specify what we are looking at and what operation we want to run on it. For\\nexample, if we wanted to list all of the records within the gamemodels collection, we would use\\nthe following command:\\ndb.gamemodels.find()\\nThis command indicates that we would like to access the game models collection within the current\\ndatabase and execute the find method on that collection. In this case, we have not specified any\\nsearch criteria, so the query will return all of the documents found within that collection. If there\\nare more than a handful, then the output will be truncated. If you read the output carefully, you will\\nsee that it instructs you to enter the itcommand to continue with the next page of results', 'Take a moment to look at the data that is returned. Here you can see the structure of the documents\\nthat are stored within this collection. The entire row of data, or the document, is returned as a JSON\\nobject. JSON objects are structured as key-value objects. This means that for each element within\\nthe collection, you will see the name of that element followed by a colon and then a value. Look at\\nthe data carefully and find one of the “levels” elements within a document. Notice that following\\nthe colon is an opening (or left) square bracket. This indicates that this element is associated with\\nan array of values rather than just one. In a similar way, we might find that an element is associated\\nwith yet another key-value element or even a collection of such elements. This is a very basic\\nexample of a nested document 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 82}),\n",
       " Document(page_content=['We can see that the structure of the data is definitely different from that of SQL. What does a slightly\\nmore complex query look like? Let’s try one', 'db.gamemodels.find(\\n{\"_id\" : ObjectId(\"5c574f5557c2a873c6ff0c89\")}\\n)\\nThis query specifies that we are looking for all documents that have a certain object ID value stored\\nwithin the _id field. The ObjectId function call found within the query is used because the ID\\nvalue present here is not simply a static string. In fact, if you were to attempt the same search using\\nthat string as the value, you would not find any records', 'You will find that in some document store databases, the ID values are stored as objects, and in\\nothers, they are not. This is not really a matter of which brand of server is used. It has more to do with\\nwhether the programmers are using the native library abstractions to interact with the database or if\\nthey are generating the data themselves. One of the places where you can find some inconsistency\\nis when a developer is storing an object ID as a part of an array within another document. This\\nis frequently done to simulate SQL relations. However, the developer has chosen to store it as a\\nstring rather than an object, which can have some advantages within the code while simultaneously\\ncreating some challenges in the database itself. Alternatively, you may find that they are storing\\nthese as objects, which simplifies database access while introducing complications into their code', 'The main thing to know is that IDs may not always have the object wrapper. As a rule of thumb, if\\nyou are looking at an integer field that appears to increment or a field that looks like a UUID value,\\nyou are almost certainly looking at a database ID column 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 83}),\n",
       " Document(page_content=['If you’d like to specify more than one search criteria, you need to simply include the additional\\nkey-value pairs in a separated list within the curly braces. Not only this, but you can also perform\\ncomparisons such as less than, greater than, etc. The notation for these can be counter-intuitive. A\\nkey to remembering what the structure of these queries look like is to bear in mind that it is always\\na key and a value. This means that if you would like to do some comparison against a date field,\\nperhaps finding dates that are greater than some other date, you would first specify the key, which\\nis the date field. Following the colon, you would then specify the comparison operator as the next\\nkey and the value to which it is being compared as the final value. We will look at an example of\\nthis shortly', 'Another very useful concept to understand is that of a projection . While you will run into this term\\nin various contacts within the documentation, it is always referring to how the data is presented. A\\ngood mental picture to use is that of a map. The actual coordinates on the globe of where continents\\nare, islands are, cities are, etc. When you know take that information and put it onto a map, you\\nare creating a projection of that data. Said another way, you are choosing to represent that data in\\na specific way. There might be a great deal more information that you have related to the locations\\nof places and objects, but not all of those things are visible', 'Similarly, a Mongo projection retrieves the data that your request and presents it in the way you\\ndescribe. This could be as simple as selecting which elements within the document should be dis-\\nplayed to something more complex like an aggregation of some sort. When it comes to exploring\\nthese kinds of topics, the documentation becomes absolutely critical. This is the most critical when\\nyou are working directly with the command-line interface. When working with some kind of ab-\\nstraction layer or library, the Mongo documentation is useful, but the library documentation is most\\nimportant. Most libraries will provide a variety of mechanisms to simplify aggregation operations 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 84}),\n",
       " Document(page_content=['An example of a projection is seen on the slide above. Notice that our search or selection criterion\\nis empty. As we have seen before, this means that we want to see all the documents within the\\ncollection. However, after the selection criteria, we have included another argument. Within curly-\\nbraces, we have listed the names of the fields or elements that we wish to see. Each of these is listed\\nas a key, which is what they truly are within the document and a value of one. Passing this value of\\none indicates that we wish to see this value in the resulting projection', 'A field of interest that we have included in our projection is the __v. This field represents the\\nversion of the document. This field can be very useful when examining the data to determine how\\nfrequently it has been changed. Within a database cluster, it is quite important to track changes\\nacross the cluster and ensure that all replicas are correct. The version field is used to accomplish\\nthis', 'A side point that you might find interesting is that the object IDs used within this datastore are\\ngenerated fields. The first several bytes indicate a timestamp, the next three bytes are a portion of\\nthe MAC address of the database server storing the data, two of the bytes represent the process ID\\nof the Mongo process, and the remaining bytes are an incrementing value. This might not be of\\nimmediate relevance, but knowing that there is a structure to this field can be useful when doing\\ntroubleshooting or trying to identify data that came from the same source or was inserted at the same\\ntime 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 85}),\n",
       " Document(page_content=['We previously mentioned that you could use comparison operators in your selection criteria. An\\nexample of this is pictured above. Note that we still include the name of the field that we wish to\\nperform the comparison against, and this field appears as the key in the selection. The comparison\\noperator, which is usually some abbreviation and includes the $, is contained within curly braces\\nalong with the value for comparison. The $can appear unusual, and you might believe that it\\nindicates an internal operation of some sort, but this is not the case. Instead, this $indicates that\\nthis will be an interpolated value that is processed at run time rather than a static field or value. You\\nwill see this sort of construct again when building aggregations', 'Also, in this case, you can see that we have specified different values for the projection. In this case,\\nwe have requested that the name field and the value field be displayed. Think about that while you\\nlook at the results returned. Is there something missing? Is there something extra? The answer to\\nboth questions is yes', 'Notice that even though it was not requested, the ID column is returned. It is always returned unless\\nyou explicitly set it to zero in the projection. Additionally, you should notice that there is no name\\ncolumn displayed. Why is this? The simple answer is there is no name field within the data. Even\\nso, it is very interesting that there are no errors generated! If you tried this same kind of thing with\\na SQL database, you would absolutely have an error', 'This can be a very interesting feature when our data is not homogeneous. In other words, imagine\\nthat a change of database design has happened and some records have a name field while others\\ndo not. In a SQL database, this can never happen. All of the rows must be consistent. Within\\na document store, this is perfectly legal. This allows us to retrieve records regardless of how the\\ndesign may have changed or which fields are present', 'Because of this, if the field isn’t present, there is no output for that field, and there is no error. While\\nconvenient, this can create some difficult troubleshooting problems. Imagine that when writing a\\n8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 86}),\n",
       " Document(page_content=['query, you mistype the name of a field. You could end up spending a great deal of time trying to\\nfigure out why the field is not being displayed, while the simple reason is that you have misspelled\\nit 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 87}),\n",
       " Document(page_content=['Aggregation and Relations\\nIf the data stored within the document store represents individual documents with no relations,\\nqueries are very simple. In our experience, it is rare to have data that can be represented in this\\nway within most enterprise applications. This means that you will need to run some kind of rela-\\ntional query. How can we do that if this is a non-relational database? A way to get started with an\\nanswer to this is the idea of aggregations', 'If you recall our brief discussion about map-reduce operations, you can think of the aggregation\\noperation as a reduce operation. We are taking a large set of data, or more likely just one field with\\nthat data, and we are reducing it down to a single value. Said another way, we are aggregating all\\nof the like values from a certain field and performing some simple operation on these', 'Considering the example in the slide, we are aggregating all of the value fields found within the\\nindividual question model documents. In the aggregation operation, we are asking to group all of\\nthese, assigning a new name, totalPoints , which receives the sum of the value fields. The\\noutput, as you can see, is the sum of all of those value fields. This is a good start, but it’s not quite\\na relation 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 88}),\n",
       " Document(page_content=['To accomplish this, we will write a much more complex query. We are still performing an aggre-\\ngation of a sort since we are trying to collect multiple records in some way. The first thing that we\\npass into the selection or filter position is an $unwind operator. Notice that we are passing it a\\nvalue of $questions . Where does this value come from? This is the interpreted value of the\\nquestions field within the current section model as the query iterates over the collection', 'Consider the next argument passed. Here we find a $lookup operator. Within this lookup is a\\nrelation. We are requesting that the values within questions , which is an array containing the\\nobject ID values of the individual question records related to the section model, are used to select\\nthose records from the question models collection using the _id field within that foreign collection\\nas the join criteria. Each resulting question that is returned will be placed temporarily into the\\nquestion variable', 'The$match argument is being used to identify which section model documents should be used\\nto perform in this join. The final argument generates the projection. In this case, we are asking\\nto see the name the field from the section model along with the value for each question that is\\nconnected to this section 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 89}),\n",
       " Document(page_content=['MongoEngine\\nBy no means should the query from the last slide be viewed as unusual or difficult. While many\\nview the SQL language as being challenging and nonintuitive, we think everyone will agree that the\\nquery language in mongo is definitely less intuitive, especially attempting to do any kind of join or\\naggregation! Is there an easier way? We think so', 'While being able to muddle through at the command-line interface for a database is important, we\\ncertainly don’t want to do this every day. Since we are looking to automate as much of our work\\nas we can, it makes sense to spend time looking at ways to create abstractions using our chosen\\nlanguage. MongoEngine is an excellent Python library that provides high-level object-oriented\\nabstractions to the underlying Mongo database', 'Using MongoEngine, a relation such as what we just performed, can be dramatically simplified', 'Aggregations are also much more straightforward, and iterating over the data is very simple since\\nwe can use list comprehensions and other Python constructs', 'To make good use of this library, though, we have to have the Python objects that represent the\\nstructure of the Mongo database. At first, this might sound insurmountable, but it is not actually that\\ndifficult. MongoEngine makes use of a lower-level library called PyMongo. We can use PyMongo\\ndirectly to interact with the database much as we would from the command line. Using either the\\ncommand line or PyMongo, we can determine the names of databases, names of collections within\\ndatabases and retrieve arbitrary documents. We can use these to reconstruct the Python objects This is much easier than it sounds 8'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 90}),\n",
       " Document(page_content=['PyMongo\\nLet’s work through the beginning of a simple example. Our first task will be to connect to the\\ndatabase. Since we do not yet know the structure of the documents or the names of the collections,\\nwe cannot yet use MongoEngine. Therefore, we will begin with PyMongo', 'After importing the library, we can create an instance of the MongoClient object. The constructor\\ncan accept several arguments, but the most important is the location of the database to which we\\nare connecting. If additional credentials are required, these can also be included as arguments', 'Unfortunately, most Mongo databases tend to be deployed with no authentication required. For this\\nreason, we aren’t going to spend a great deal of time discussing different authentication options here', 'If your database does require authentication, just refer to the PyMongo documentation to determine\\nhow to present the specific type of credentials that you have', 'Once this has been instantiated, we can begin to interact with it directly. Our first step will almost\\nalways be to call the list_database_names() method. The output depicted in the slide is\\nfairly typical. The admin ,config , andlocal databases are all internal to Mongo. That means\\nthat the data of interest must be stored in scoreserver  9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 91}),\n",
       " Document(page_content=['Now that we know the name of the database we are interested in, we need to tell the database driver\\nthat we want to use it. At the command-line interface, we would use the use <database>\\ncommand. Here, we simply refer to it as an attribute of the connection object. While this might not\\nbe obvious or intuitive, it is definitely easy! In the slide, we capture this reference to the score-\\nserver database into a new variable', 'Now that we have this reference, we can use it to interrogate the database and access collections. In\\nour case, we use the list_collection_names() message to retrieve an array of all of the\\ncollections present in this database 9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 92}),\n",
       " Document(page_content=['You are likely beginning to detect a pattern. First, we determine the names of the databases and then\\nselected one to use. Using that, we determine the names of the collections. Finding a collection of\\ninterest, we can now refer to it directly. Here we are capturing a reference to the usermodels\\ncollection and accessing it as though it is a key in a dictionary named db', 'Now that we have a reference to the usermodels collection, we can begin to interact with it. At\\nthis point, you can see something that probably looks familiar! Note the find_one() function', 'This is the same function that we would use at the Mongo command line. In fact, the goal of\\nPyMongo is to give you access to the native functions available within the Mongo database server 9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 93}),\n",
       " Document(page_content=['On the previous slide, we ran a query to find one document out of the user models collection. The\\nslide shows us this record. We can see that the record is in the expected JSON notation, and we can\\nidentify the keys and values. We can also see that there are some nested subdocuments or values\\npresent', 'Several of the fields are particularly interesting as examples of things that we have mentioned pre-\\nviously in this section. Specifically, notice the newGameArray key and value(s). This field\\ncontains an array. That array appears to contain a dictionary, though it is just another object from\\na JavaScript point of view. The first field in this dictionary, gameId , contains an object ID value', 'This is a very obvious indicator that this is a reference field to another collection. This is reinforced\\nby the fact that there is also an _id field that also contains an object ID', 'Continuing to the next field, we find gameData . This also contains another dictionary. Look\\ncarefully at the keys in this dictionary. Do they look familiar? Compare them to the object IDs also\\nfound within this document. These are object IDs! However, these do not have the ObjectId() -\\nclass wrapping them. Instead, they are bare strings that are being used as dictionary keys', 'While this is a good start, how can we easily access the status? After all, while we can see this\\nstructure, we do not have matching Python objects. Let’s try to solve this 9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 94}),\n",
       " Document(page_content=['From PyMongo to Document Models\\nIf we decide that we care about the usermodels collection, we can take the time to reverse\\nengineer a document structure that is close enough to allow us to access this data easily. Don’t be\\noverly concerned about getting it exactly right. It really does not need to be! It simply needs to be\\nclose enough for the data types to be acceptable. You also do not have to be overly concerned about\\ngenerating matching fields for every single element in the data. For example, this document has\\na somewhat complex structure with several fields containing arrays, collections of sub-documents,\\nor other objects. Rather than trying to define these fully, we can simply designate them as an array', 'To make this class, which you can think of as a crosswalk mapping, begin by looking at the object\\nreturned by the PyMongo call. Begin by identifying all of the top-level field names and consider the\\nvalues assigned to them to make a rough guess about data types. If you are unsure, a string type can\\nbe used to contain almost anything. With this initial examination done, we need only define a class\\nthat is a subclass of the MongoEngine Document class. Within the course, we define instance\\nvariables with names that match the fields within the Mongo document 9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 95}),\n",
       " Document(page_content=['Here we have followed the pattern that is described on the previous slide. Looking at the user-\\nmodel document that was returned, we can see that there is a name field that seems to contain a\\nstring. Similarly, there is a password field that also contains a string', 'The next two fields look more complex. Rather than trying to parse these completely and to de-\\nfine their content precisely, let’s just define them as arrays of strings. The way that we do this\\nwithin Mongo and, therefore, MongoEngine, is to define them as ListField objects that contain\\nStringField() objects', 'Next is a rights field that seems to only contain integer values, so we will define that as an\\ninteger. The next two fields are the sessionId and the teamId , which both appear to contain\\nObjectId() values. However, for now, at least, let’s define these as strings', 'The next field, newGameArray , is also a very complicated set of nested documents. To simplify\\nthings, we will define this as an array of strings. The last field that we are going to try to capture is\\ntheupdated field, which clearly contains a date, so we will define this as a DateTimeField() ', 'Now that we have a definition of this document, we can use it to access data within the collection', 'Switching over to the MongoEngine style, we establish a connection to the Mongo server, simulta-\\nneously selecting the scoreserver database as our focus. Once that is done, we can simply ask\\nfor the first object in the usermodels collection and extract the name. When we do this, we see\\nthat we have successfully retrieved a record, and it has been populated, allowing us to access the\\nname of the first user in the collection. Success!\\n9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 96}),\n",
       " Document(page_content=['Stand back and think about why we are doing what we are working on now. If we have a collection\\nof data that simply contain strings, we could just query that directly and not worry about creating\\na matching Python class. While it might be a bit unusual, simple queries with this kind of data are\\nstraightforward in Mongo. What if I have document collections that are somehow using relations\\nsuch as might be present in SQL?\\nIn the case where collections are related, and those relations matter to us for data acquisition, it\\nmight be well worth our while to generate matching Python classes', 'This can drastically simplify\\nthe use of those relations. Similarly, if the relations are across multiple databases rather than just\\nbetween collections, we would likely make the same choice. You really need to do a cost-benefit\\nanalysis of the effort involved. Ask yourself how difficult the query will be to write using the native\\nquery language. Also, ask yourself how many times you were going to need to transform the data', 'If you find yourself doing it more than one time or needing to create more than one relation, or if\\nthat relationship is tricky, the time to create this abstraction is early ', 'On the other hand, if all of the data is within a single collection, or if minimal transformations of\\nthe data as stored will be required, then we probably would not want to create this data abstraction', 'As a rule of thumb, when you find yourself diving into the Mongo documentation to figure out how\\nto run a query, and it is not making intuitive sense to you, you should pause and ask if you should\\nbe re-creating this using a document abstraction and Python 9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 97}),\n",
       " Document(page_content=['We are certain that you recognize that the Mongo database landscape is very large and can be quite\\ncomplex. Just as was true with our SQL section, our goal is not to make you a database administrator', 'One of our very first tasks, and the main task that we are focusing on in this volume, is the acquisition\\nof data. We should be able to do this on our own to a reasonable degree. While we need not be\\nexperts, we should know how to interact with document stores effectively', 'The lab that follows is going to force you to interact with a Mongo database. In fact, the database\\nthat you will work with is the same data that has been pictured here in the slides and used in demon-\\nstrations. We will also make some use of this data tomorrow for some basic statistics. In our final\\nsection of the day, we will learn to interact with Mongo a little bit differently, using it as an easy-\\nto-use storage system scraping websites. More on that soon 9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 98}),\n",
       " Document(page_content=['Lab 4: Querying Document Stores\\nPlease pick up with your labs wherever you left off. Hopefully, you are ready to begin working on\\nthe Document Store lab. If so, open the appropriate notebook from within Jupyter to get started', 'This lab will also require the use of the virtual machine. The Mongo database that you will interact\\nwith is running on that virtual machine 9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 99}),\n",
       " Document(page_content=['Web Scraping\\nOur final section of this book will deal with another very common source of data used by data\\nscientists every day: web scraping!\\n9'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 100}),\n",
       " Document(page_content=['When the topic of web scraping comes up, people most often think about connecting to existing\\npublic websites and extracting data from them. This has been such a huge load on some websites,\\nin addition to the outright theft of material, that many websites take steps to make web scraping\\nvery difficult. While this is certainly one of the meanings of web scraping, there are many other\\ncontexts in which web scraping occurs', 'Consider your own organization. Are there any internal web applications that are used to create\\nuseful dashboards? Is there a date represented on any of these dashboards that might prove to be\\nuseful for some sort of statistical analysis or perhaps a machine learning solution? For example,\\none of our clients uses kinetic uninterruptible power supplies or flywheel UPSes. These devices\\nhave a JSON-based API that exposes dozens of metrics about the performance of the UPS', 'These\\ninclude things like current, RPM, average RPM, temperature, vacuum pressure, etc. After having\\nseveral of these devices fail unexpectedly and with no notice, the organization began to harvest all\\nof the data exposed through the API. Once they had collected several months’ worth of data and\\nhave observed several other devices fail, they were able to build a correlation model that could be\\nused to predict when the next UPS would fail. The model was so good that it could typically predict\\nthe failure of a UPS to within a 30-minute window 72 hours in advance', 'With this information in hand, our customer was able to convince the vendor to begin quarantine\\nreplacements of UPSes that have not yet failed based solely on their predictive model. This is\\na wonderful example of Data science being used to prevent a security incident from occurring', 'Availability incidents are not normally the kind of incidents that we think about daily. What does\\nthis have to do with web scraping? Remember, the API exposed a JSON interface. This means web\\nscraping was used!\\n10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 101}),\n",
       " Document(page_content=['HTML Basics\\nWhile we would like to assume that everyone knows how HTML works and what it looks like, that\\nseems like an unsafe assumption for our class. Therefore, if you already have some background in\\nHTML and you are familiar with it, please be patient while we take just a few moments to review\\nthe bare-bones basics', 'HTML is a text-based markup language that was created in the early 1990s. A markup language is\\nsimply some kind of notation or symbolic language for annotating or representing the structure of\\ntext. You can think of what proofreader marks might look like in typewritten text. Certain marks\\nmean that things should be bold, other marks mean words should be deleted, yet others mean that\\nwords should be inserted. In a similar way, HTML provides marks that can be used to indicate how\\ntext should be rendered', 'These marks are referred to as tags. Each tag is contained within angle brackets, or less than and\\ngreater than signs. Nearly all of the tags come in pairs. This is convenient because it makes it\\nsimple to identify where blocks of related data are. For all the tags that come in pairs, the closing\\ntag is prefixed with a forward slash. An example would be <tag> indicating the beginning of an\\narbitrary tag and </tag> representing the closing tag', 'Most tags can take many different arguments that are used for styling using CSS or possibly to make\\nthe page more reactive using JavaScript. While we are not particularly concerned about CSS, the\\nCSS identifiers and names can be useful when trying to extract the data. For example, if we are\\ntrying to identify the headline text for news articles and we find that the HTML content has a CSS\\nidentifier named News headline, that might be the most efficient way to identify and extract the\\nheadlines 10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 102}),\n",
       " Document(page_content=['We are not going to try to provide you with an exhaustive list of HTML tags. There’s really no\\nvalue in this since we are not trying to generate HTML pages, and our interest in HTML does not\\nextend beyond how we can extract the data out of the HTML page. If you are curious, the W3C\\nmaintains the standards and documentation for the official tags supported under the standards. Even\\nthis is not complete, however. Every browser we have ever seen will also implement proprietary\\ntags. This started some years ago as browser vendors attempted to establish market dominance by\\nhaving features that force you to use their browser should sites implement these non-standard or\\nproprietary tags', 'In the past, any non-standard tags would result in an error message to the user. The creation of\\nproprietary non-standard tags proliferated so widely that the browser vendors have uniformly ceased\\nissuing these warnings. Today, if a browser sees a well-formed tag that it cannot identify, the tag is\\nsimply ignored, along with all of the content within it. As an aside, this might mean that there is\\nmore information present in the webpage than the web browser is rendering', 'SPAs1, and other responsive web applications that make use of custom stylesheets and JavaScript\\nlibraries rely heavily upon non-standard tags. Since browsers now ignore them, they can be used\\nto safely identify different portions of the application within the DOM, or document object model,\\nthat can be changed dynamically by the JavaScript as the user interacts with the application', '1“Single Page Applications” are a form of responsive web application that presents the appearance and responsive-\\nness of a local desktop application or mobile app while actually being delivered as a web application. Rather than links\\nand buttons causing the browser to navigate to different pages, all of the content is rendered through this single page 10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 103}),\n",
       " Document(page_content=['When most programmers first interact with HTML with the goal of extracting data, they will usually\\nintuit that regular expressions are probably the way to go. While regular expressions are very pow-\\nerful and can easily solve most problems, parsing HTML and, more generally, XML with regular\\nexpressions can be exceptionally challenging. As a result, we would strongly dissuade you from\\nattempting to do so. It is far easier to make use of a well-documented library that implements an\\nHTML parser', 'If we use such a library, it can be quite simple to extract all of the links, for instance. Or to extract\\nall of the image tags. Or perhaps there is an element or two that has a specific CSS class or ID These libraries make this type of access very easy', 'Pictured in the slide, we see a basic example of a simple webpage. While not required, most web-\\npages will begin with a doctype . This defines how this document should be parsed by the browser', 'Following this, we have an HTML tag. If you look at the bottom of the content, you will find that\\nthere is a matching closing tag with the leading forward slash. Between these two tags sit all of the\\ncontent that will be rendered as the webpage', 'This idea of opening and closing tags provides a simple way of organizing sections of the document', 'Notice that the third line of the document begins with the head tag. Can you find the closing tag?\\nIt’s located at the end of the fifth line. While developers will typically structure their document\\nusing newlines and tabs to indicate semantic structure, there is no requirement to do so. You could,\\nin fact, put the entire content of the webpage on a single line', 'Many of the elements that we will find within these webpages are of no real interest to us. For\\nexample, the icon used in the browser bar, the stylesheet referenced on line five, the JavaScript\\nfound on the next to the last line. We are usually concerned with the textual content 10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 104}),\n",
       " Document(page_content=['SPAs and Scraping\\nSingle-page applications are specifically designed to simulate a desktop-based experience within a\\nweb browser. While these applications are wonderful, they can represent a significant challenge if\\nwe need to scrape data. The primary reason is that the JavaScript must be navigated in order to get\\nat the data that we need', 'Consider the simple webpage on the slide. Here we can see the beginning HTML tag, the head\\ncontaining the title, and the body. The body contains only two things. The first is a non-standard\\ntag, and the second is in reference to the JavaScript source code that should be loaded. Where is\\nthe web content? The answer is, this is it', 'A single page application will set up a very simple document model, in this case just the heading\\nand the body with perhaps one non-standard tag, and then use JavaScript to generate all of the\\nHTML that will be displayed to the user. It is possible for everything required to render the page\\nto be present in the JavaScript, but not all applications will work in this way. More frequently, the\\napplication will make queries to the backend and, based on those responses, either display data that\\nhas been returned or perhaps even render additional JavaScript it has been delivered, creating new\\ndocument object model elements', 'If we are trying to create a web scraping script, it is clear that this creates a problem for us. When\\nwe retrieve the URL using Python and then pass the results into the web scraping library to parse\\nthe HTML, there might be no data present at all! How can we get at the data that we need?\\n10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 105}),\n",
       " Document(page_content=['There are several ways to approach this problem. One of these is the use of the Selenium library', 'Selenium is a web driver that can be used programmatically. You can think of this mentally as an\\nin-memory browser with no visible browser window. We can interact with this rendered browser\\npane through our Python script. This means that if we need to allow the DOM to be built and then\\nclick on things that have been rendered, Selenium is a very effective way to do that', 'In our experience, this will always require some tinkering. Obviously, every webpage is different', 'Significant trial and error are frequently required to make sure that we understand precisely how\\nthe page is rendered in our browser and how it is rendered in Selenium. Next, we need to identify\\nreliable ways to identify the elements with which we need to interact with programmatically. The\\nrendering engine used within Selenium is based on chrome 10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 106}),\n",
       " Document(page_content=['There might be a better way. Selenium is certainly a straightforward way to address the problem\\nthough it can be very heavy-handed. What we mean is that we are now running a web driver to\\nrender pages in memory so that we can access them programmatically through Python. That is a\\nlot of work if all we really want to do is extract some data. It also opens us up to the possibility\\nthat this more complex approach and solution can end up resulting in unexpected failures if we are\\ntrying to build something that will periodically query some sort of web-based data store within our\\nenvironment in order to populate it into a database or make some continuous predictions', 'It might be possible to eliminate Selenium and the need to render the webpage. How so? Our goal in\\nusing Selenium was to ultimately retrieve the data of interest by allowing the JavaScript to populate\\nthe document object model under Selenium, interacting with the page programmatically as needed\\nto further update the DOM until the data of interest is rendered into the DOM. If all we are really\\ntrying to do is capture the data that is returned in the API, could we just query the API ourselves?\\nThat seems like a reasonable approach', 'Getting started doing this turns out to be pretty simple. The easiest way to approach it is to simply\\nuse a browser with the developer console open. We will use chrome in our discussions since it is\\nubiquitous. Using the chrome developer console, we can look retrospectively at all queries that\\nhave occurred and what the responses were. If I know which data I am trying to extract, I can look\\nthrough those queries or search them to find the query that resulted in the output that I need. With\\nthis done, it is a simple matter to copy the query that was sent and to re-create that in Python', 'This approach has another huge benefit. If we identify and interrogate the API directly, it is highly\\nlikely that the API is returning the data as a JSON object. Certainly, other formats can be used', 'XML is another format that you might find in enterprise SOAP-based applications. These XML\\ndocuments can be processed in the same way that our webpages would be, and JSON objects can\\nbe digested directly 10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 107}),\n",
       " Document(page_content=['Since Selenium is not ideal, especially in a long-running process that we wish to use to retrieve,\\nparse, and process data out of an API, we are faced with the question, “How are we to eliminate\\nSelenium?” A possible solution is to use the developer console in a web browser to identify the\\nqueries of interest and send them directly. This also eliminates the need to parse the web content\\ncompletely', 'If we consider Chrome as a possible solution, begin by accessing the site or URL of interest. Perform\\nany necessary authentication and locate the link that leads to the data of interest of the URL of the\\npage that displays the data of interest. With this completed, open the developer tools window. This\\nis typically accomplished with the F12 key. The developer tools can be broken out of the web\\nwindow, but we have chosen to leave it connected in the screenshot above', 'In the case we are considering, we are looking at a stock ticker for IBM published by Google. The\\nstock quote is periodically updated automatically by JavaScript code running in the web browser', 'Using the Network tab in the developer console, we have located one of the query/response pairs\\nthat returns the current stock quote inside the response. The appearance of that response, in this\\ncase, is not ideal since we are using the Preview option rather than viewing the raw Response. We\\nhave only done this to make it easier to see the data in the response. In reality, we would be parsing\\nthe returned JSON, or other text, that is returned by the API', 'Now that we have located one of the requests that returns the data of interest, we could right-click\\non it and open it in a new window. This allows us to verify that requesting that URL will return the\\ndata we wish to scrape. Additionally, it is a useful way of obtaining the complete query required\\nwithout having to dig through the JavaScript code on the page. Assuming that the result in the\\nnew tab contains the data of interest, we can now simply copy the entirety of the request, including\\nany required headers, cookies, or tokens, to our Python code. We can now generate a request and\\nexamine the result. This result can then be parsed, as needed, if it is not a JSON result 10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 108}),\n",
       " Document(page_content=['You will have an opportunity to experiment with some of these tasks in the next lab. We’re not\\nquite ready for it yet because we still need to talk about the HTML parsing and web request retrieval\\nlibraries', 'In the lab, you will deal with both basic retrieval and parsing of static pages and more difficult\\nHTML parsing and data extraction tasks. This will require that we have an HTML parsing library\\nand that we can make web requests. Let’s have a look at what we will use in the lab 10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 109}),\n",
       " Document(page_content=['BeautifulSoup\\nThe BeautifulSoup library is an open-source web parsing library. It is more general than this, sup-\\nporting parsing of other XML-based languages. While it is not a part of the Standard library for\\nPython, it is absolutely the most common library used for this type of work in Python', 'BeautifulSoup can ingest any HTML encoded data and represent it as an object tree. This can be\\nused to then search and access any part of the page where the content is within it. This library will\\nnot handle the retrieval of the webpage. That is your responsibility. While at first, this might seem\\nlike a limitation, it makes this more flexible. It means that this library does not care what the source\\nof your document is. Perhaps it’s a document that you have retrieved from a web server, or perhaps\\nit’s a document that you have retrieved from the file server. The source of the document doesn’t\\nmatter', 'Once you have asked this library to digest the file, you can do lookups based on tags, tag values,\\nor almost any other identifiable attribute within the document. The tree is also exposed via a set of\\narrays so that you can iterate over different portions of the tree or tags as needed', 'Do not go too quickly past the support for XML. If you have ever had to do XML parsing, then\\nyou are likely familiar with the fact that different people and organizations will represent the same\\nXML data in different ways. At first, this can seem quite surprising since XML is intended to be a\\nway to standardize the representation of data. Unfortunately, the flexibility of the language can lead\\nto some hard choices. This flexibility in representation can make XML parsing very challenging BeautifulSoup simplifies all of this 10'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 110}),\n",
       " Document(page_content=['To make use of the library we must, of course, import it. The package name is BS4. From this, you\\ncan import the BeautifulSoup class. With that done, Parsing the webpage could not be easier. We\\nsimply retrieve the webpage content from our source, whether that is a website file or perhaps the\\npage we have built in memory, and then ask BeautifulSoup to instantiate the class with that content', 'You can now instantly access anything within the content in a large variety of ways', 'In the slide, you can see that we have retrieved a Wikipedia article. After we have done this, we ask\\nthe library to parse the content. Next, we ask for a list of all of the H1tags using find_all() ', 'This accessor method returns an array with all matching tags. To extract the first one, we ask for\\nthe item at offset zero. Finally, we ask this to be represented as a string, resulting in the output\\ndisplayed at the bottom of the slide 11'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 111}),\n",
       " Document(page_content=['Let’s make this more practical. Let’s imagine that we want to create some kind of machine learning\\nor other solution to identify phishing emails that we are receiving on our email server. Certainly, we\\nhave endpoint protection software and virus scanning software on our mail server; but we’d like to\\nbe a bit more proactive and possibly identify new, unknown, but possibly suspect phishing emails\\nbefore users click on them', 'After hunting around on the internet, we manage to find a repository containing several hundred\\nexamples of phishing emails. We would very much like to obtain a copy of these so that we can do\\nsome further processing on them later in our class. The problem is that the emails are spread out\\non a number of pages on the webserver. There is no easy-to-use download link that has all of the\\nexamples in one archive. This is a perfect problem for web scraping', 'We begin by identifying the URL pattern where the data of interest is found. In our case, everything\\nis found on a page within the archive directory. With a little checking, we determined that archive\\npage one has nothing useful on it. We also determined that the very last page in the archive is page\\n331. So let’s create a for loop to iterate over the range of pages from 2 through 331. For each\\nof these numbers, we will retrieve the matching archive page from the website. After retrieval, we\\nwill process that using BeautifulSoup', 'Having examined the archive pages in our web browser, we determined that the contents of the\\nphishing emails are all contained within blockquote elements. Further, these elements do not\\nappear to be used anywhere else within the website. This is perfect for us. We now extract all of\\ntheblockquote elements as an array. With this array in hand, we can iterate over each one of\\nthem, extracting the email content 11'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 112}),\n",
       " Document(page_content=['What if the data that we need is accessible over an API? Perhaps we have decided that using Se-\\nlenium is too much work or not a good decision long-term. In this case, we need to send the web\\nrequest to whatever the API endpoint URL is and retrieve the result. This seems very simple, but a\\nlittle bit of care is required', 'When examining the API call within the developer console of something like Chrome, pay careful\\nattention to the cookies that are being sent. Specifically, we need to identify if there is some sort\\nof authentication token, Session ID, JavaScript web token, or other important element being sent in\\nthe request that will be required for our programmatic requests to function', 'One simple approach to isolating this is to right-click on the request. This will usually give you the\\noption to open it in its own web browser window. Opening the request in a new window can be\\nuseful to see if simply re-requesting the data in the same way results in the retrieval of the data of\\ninterest. In rare cases, you will find that the request can only be made one time, having some sort of\\nsingle-use token attached. In this case, we might fall back on Selenium rather than trying to work\\nout how that single page request is keyed', 'We may also determine that the API call makes use of a POST request. Sending a POST request\\ncan seem very challenging, but it turns out to be easy. What we need to do is URL-encode whatever\\ndata it is that we need to send. Once we do this, embedding things like cookies and other elements,\\nwe can include the encoded data as a second argument to the urlopen() request. This will\\nautomatically send a POST request rather than a GET. With this done, we can now extract the\\nresulting JSON object and continue with our processing 11'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 113}),\n",
       " Document(page_content=['Lab 5: Web Scraping\\nIt’s time to start our last lab for this book. In this lab, you are going to interact with a Web server\\nusing a variety of tools. You will look at doing basic web scraping from simple pages. You will\\nalso re-create the extraction of the phishing email content. Further, after extracting this content, you\\nwill also learn how to insert that data into a new Mongo database. This turns out to be pretty useful;\\nsince we will often be extracting large amounts of data, we need somewhere to store temporarily\\nyet efficiently. While we could use a flat-file1, think about the challenges of trying to decide how\\nto store your phishing emails in some kind of CSV file. Storing it into a database turns out to be\\nmuch easier', '1A flat-file is essentially just a plain text file. A CSV file is an example of a flat-file 11'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 114}),\n",
       " Document(page_content=['Are you ready for a challenge? Each day of the course, we have included one or more challenges\\nfor you to complete on your own. None of these challenges are required to complete the course Instead, these are intended to be personal exploration projects', 'All of the challenges expand on one or more things covered during that portion of the course (and\\nmay draw on things from previous sections). They are intended to give you projects to work on\\nwhen you have time, each of which will hone your skills, expand your knowledge, and sometimes,\\nimprove on a solution given in the course', 'Today’s challenge is fairly straightforward. In the MongoDB lab, we worked on a solution that\\nwould reverse engineer an existing MongoDB database and allow us to run queries against it. Your\\nchallenge is to take that code as a starting point and expand it to answer a specific question:\\nWhich question do the majority of users seem to have the most difficulty answering? In other words,\\nwhich question do most students answer incorrectly at least one time?\\n11'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 115}),\n",
       " Document(page_content=['Conclusion\\nHopefully, you have enjoyed your experience so far. Undoubtedly, this experience has been quite\\ndifferent from other science classes that you have taken. First of all, the book is quite small! Instead,\\nthe majority of your time has been spent working through labs after some basic instruction and\\ndemonstrations from your instructor or the pre-recorded content', 'The key points to take away from today are about the fields of data science in general and our\\npurposes in this applied data science course. All of the techniques and tools that we worked with\\ntoday will be used throughout the rest of the course. Everything that we do each day builds on the\\nthings taught in the previous day', 'This means that in future labs, we will assume that you know how to interact with a Mongo database,\\na SQL server, or perform a web query. If you have challenges with any of these things, you should\\nstop and ask your instructor. Make sure that you’ve mastered these topics. It is also important that\\nyou have completed all of the labs connected to this book before proceeding into the next book', 'To that end, our labs included a Python bootstrap to make sure that you have at least enough basic\\nPython knowledge to be successful in the rest of the class. We will use some Python constructs\\nthat are a bit more complicated than what was covered in the bootstrap, but those things should be\\nintuitive to pick up based on the progression through the remainder of the material. We also spent\\ntime learning a little bit about how to visualize data with matplotlib. While there are many other\\nlibraries that are used for rendering graphics, this library is very frequently the underlying library\\nin these other tools. Understanding and being able to use this fluently is an important step in being\\nable to represent our data in meaningful ways that others can gain insight from', 'Finally, we looked at three different ways that we can acquire data. That could be from SQL Servers,\\n11'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 116}),\n",
       " Document(page_content=['document stores like Mongo, or web scraping. With the web scraping section, we also dealt with\\ninterrogating web-based APIs. While we will not have the occasion to do so in the class, you can\\ntranslate the API query information directly into arbitrary network interrogation fairly simply. There\\nis yet another mechanism that we might view that we will examine tomorrow 11'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 117}),\n",
       " Document(page_content=['This page intentionally left blank 11'], metadata={'source': 'decrypted_SEC595 - Book 1_2036060.pdf', 'page': 118})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import importlib\n",
    "import workers\n",
    "importlib.reload(workers)\n",
    "import workers\n",
    "\n",
    "\n",
    "def apply_parallel(data):\n",
    "    # Use all available cores\n",
    "    pool = Pool(processes=multiprocessing.cpu_count()-1)\n",
    "    result = pool.map(workers.process_book, data)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return result\n",
    "\n",
    "if __name__ ==  '__main__': \n",
    "    processed_data = apply_parallel(sec595)\n",
    "\n",
    "processed_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[373.4,\n",
       " 311.7142857142857,\n",
       " 1673.0,\n",
       " 145.3846153846154,\n",
       " 338.25,\n",
       " 593.3333333333334,\n",
       " 472.0,\n",
       " 297.4,\n",
       " 380.5,\n",
       " 367.2857142857143]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get average length of the pages for each book\n",
    "avg_page_lengths = []\n",
    "\n",
    "for page in processed_data[7]:\n",
    "    avg_page_lengths.append(sum([len(pg) for pg in page.page_content]) / len(page.page_content))\n",
    "\n",
    "avg_page_lengths[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loader that loads data from JSON.\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Callable, Dict, List, Optional, Union, Any\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.document_loaders.base import BaseLoader\n",
    "\n",
    "\n",
    "class JSONLoader(BaseLoader):\n",
    "    def __init__(\n",
    "        self,\n",
    "        file_path: Union[str, Path],\n",
    "        content_key: Optional[str] = None,\n",
    "        metadata_func: Optional[Callable[[Dict, Dict], Dict]] = None,\n",
    "        text_content: bool = True,\n",
    "        json_lines: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the JSONLoader with a file path, an optional content key to extract specific content,\n",
    "        and an optional metadata function to extract metadata from each record.\n",
    "        \"\"\"\n",
    "        self.file_path = Path(file_path).resolve()\n",
    "        self._content_key = content_key\n",
    "        self._metadata_func = metadata_func\n",
    "        self._text_content = text_content\n",
    "        self._json_lines = json_lines\n",
    "\n",
    "    def load(self) -> List[Document]:\n",
    "        \"\"\"Load and return documents from the JSON file.\"\"\"\n",
    "        docs: List[Document] = []\n",
    "        if self._json_lines:\n",
    "            with self.file_path.open(encoding=\"utf-8\") as f:\n",
    "                for line in f:\n",
    "                    line = line.strip()\n",
    "                    if line:\n",
    "                        self._parse(line, docs)\n",
    "        else:\n",
    "            self._parse(self.file_path.read_text(encoding=\"utf-8\"), docs)\n",
    "        return docs\n",
    "\n",
    "    def _parse(self, content: str, docs: List[Document]) -> None:\n",
    "        \"\"\"Convert given content to documents.\"\"\"\n",
    "        data = json.loads(content)\n",
    "\n",
    "        # Perform some validation\n",
    "        # This is not a perfect validation, but it should catch most cases\n",
    "        # and prevent the user from getting a cryptic error later on.\n",
    "        if self._content_key is not None:\n",
    "            self._validate_content_key(data)\n",
    "        if self._metadata_func is not None:\n",
    "            self._validate_metadata_func(data)\n",
    "\n",
    "        for i, sample in enumerate(data, len(docs) + 1):\n",
    "            text = self._get_text(sample=sample)\n",
    "            metadata = self._get_metadata(sample=sample, source=str(self.file_path), seq_num=i)\n",
    "            docs.append(Document(page_content=text, metadata=metadata))\n",
    "\n",
    "    def _get_text(self, sample: Any) -> str:\n",
    "        \"\"\"Convert sample to string format\"\"\"\n",
    "        if self._content_key is not None:\n",
    "            content = sample.get(self._content_key)\n",
    "        else:\n",
    "            content = sample\n",
    "\n",
    "        if self._text_content and not isinstance(content, str):\n",
    "            raise ValueError(\n",
    "                f\"Expected page_content is string, got {type(content)} instead. \\\n",
    "                    Set `text_content=False` if the desired input for \\\n",
    "                    `page_content` is not a string\"\n",
    "            )\n",
    "\n",
    "        # In case the text is None, set it to an empty string\n",
    "        elif isinstance(content, str):\n",
    "            return content\n",
    "        elif isinstance(content, dict):\n",
    "            return json.dumps(content) if content else \"\"\n",
    "        else:\n",
    "            return str(content) if content is not None else \"\"\n",
    "\n",
    "    def _get_metadata(self, sample: Dict[str, Any], **additional_fields: Any) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Return a metadata dictionary base on the existence of metadata_func\n",
    "        :param sample: single data payload\n",
    "        :param additional_fields: key-word arguments to be added as metadata values\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if self._metadata_func is not None:\n",
    "            return self._metadata_func(sample, additional_fields)\n",
    "        else:\n",
    "            return additional_fields\n",
    "\n",
    "    def _validate_content_key(self, data: Any) -> None:\n",
    "        \"\"\"Check if a content key is valid\"\"\"\n",
    "        sample = data.first()\n",
    "        if not isinstance(sample, dict):\n",
    "            raise ValueError(\n",
    "                f\"Expected the jq schema to result in a list of objects (dict), \\\n",
    "                    so sample must be a dict but got `{type(sample)}`\"\n",
    "            )\n",
    "\n",
    "        if sample.get(self._content_key) is None:\n",
    "            raise ValueError(\n",
    "                f\"Expected the jq schema to result in a list of objects (dict) \\\n",
    "                    with the key `{self._content_key}`\"\n",
    "            )\n",
    "\n",
    "    def _validate_metadata_func(self, data: Any) -> None:\n",
    "        \"\"\"Check if the metadata_func output is valid\"\"\"\n",
    "\n",
    "        sample = data.first()\n",
    "        if self._metadata_func is not None:\n",
    "            sample_metadata = self._metadata_func(sample, {})\n",
    "            if not isinstance(sample_metadata, dict):\n",
    "                raise ValueError(\n",
    "                    f\"Expected the metadata_func to return a dict but got \\\n",
    "                        `{type(sample_metadata)}`\"\n",
    "                )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for i, books in enumerate(processed_data):\n",
    "    for j, page in enumerate(books):\n",
    "        for pg in page.page_content:\n",
    "            docs.append({\n",
    "                \"page_content\": [pg],\n",
    "                \"metadata\": {\n",
    "                    \"source\": page.metadata['source'],\n",
    "                    \"page\": page.metadata['page']\n",
    "                    }\n",
    "            })\n",
    "with open('docs.json', 'w') as f:\n",
    "    json.dump(docs, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loader = JSONLoader(\n",
    "    file_path='docs.json',\n",
    "    text_content=False)\n",
    "\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3708"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#finding-your-cloud-id\n",
    "ELASTIC_CLOUD_ID = getpass(\"Elastic Cloud ID: \")\n",
    "\n",
    "ELASTIC_URL = getpass(\"Elastic URL\")\n",
    "\n",
    "# https://www.elastic.co/search-labs/tutorials/install-elasticsearch/elastic-cloud#creating-an-api-key\n",
    "ELASTIC_API_KEY = getpass(\"Elastic Api Key: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store = ElasticsearchStore(\n",
    "    es_url=ELASTIC_URL,\n",
    "    es_api_key=ELASTIC_API_KEY,\n",
    "    index_name=\"sec595\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents = vector_store.from_documents(\n",
    "data,\n",
    "es_url=ELASTIC_URL,\n",
    "es_api_key=ELASTIC_API_KEY,\n",
    "index_name=\"sec595\",\n",
    "strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(\n",
    "    model_id=\".elser_model_2_linux-x86_64\"\n",
    "),\n",
    "bulk_kwargs={\n",
    "    \"request_timeout\": 60,\n",
    "},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showResults(output):\n",
    "    print(\"Total results: \", len(output))\n",
    "    for index in range(len(output)):\n",
    "        print(output[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total results:  4\n",
      "page_content='{\"page_content\": [\"Previously, we used the Bag of W ords approach, generating a multi-hot encoded vector indicating\\\\nwhich words were present in a given text. One of the major limitations of this approach was that\\\\nwe only know that a word was present, not the order that the words appeared in. If you consider\\\\nthe two following sentences, you will appreciate why this is such a big problem:\\\\nYou did understand\\\\nDid you understand\\\\nBoth of these sentences contain the same words and they would be encoded identically under Bag\\\\nof Words. Bag of Words also doesn\\\\u2019t preserve the number of times any given word appears in a\\\\npiece of text\"], \"metadata\": {\"source\": \"decrypted_SEC595 - Workbook 2_2036060.pdf\", \"page\": 213}}' metadata={'source': 'C:\\\\Users\\\\nc\\\\Desktop\\\\projects\\\\sec595\\\\docs.json', 'seq_num': 3245}\n",
      "page_content='{\"page_content\": [\"Optimization\\\\nStill, there are some really important lessons. One of the most important is that when the system\\\\nis learning, which we should clearly understand to mean automatically updating coefficients to\\\\napproximate a function that minimizes loss, our network does not understand the movie reviews or\\\\nthe emails at all! The approach to representing our data is called Bag of Words. This name derives\\\\nfrom the fact that we are simply tracking which words are used, completely disregarding the order\\\\nthat the words are in2. Certainly, we could use this to create some type of word vectors as a way of\\\\npreserving the order, but we\\\\u2019re not going to expand this particular network at this time\"], \"metadata\": {\"source\": \"decrypted_SEC595 - Book 4_2036060.pdf\", \"page\": 46}}' metadata={'source': 'C:\\\\Users\\\\nc\\\\Desktop\\\\projects\\\\sec595\\\\docs.json', 'seq_num': 1111}\n",
      "page_content='{\"page_content\": [\"Hint:There is no need to make this list unique! In fact, we do not wantit to be unique. Remember\\\\nthat in a bag of words approach, we need to work out a dictionary that will be sorted from most\\\\ncommon to least commonly used words. To properly build this dictionary we must have all of the\\\\nwords, not just the unique words in a message\"], \"metadata\": {\"source\": \"decrypted_SEC595 - Workbook 2_2036060.pdf\", \"page\": 134}}' metadata={'source': 'C:\\\\Users\\\\nc\\\\Desktop\\\\projects\\\\sec595\\\\docs.json', 'seq_num': 2861}\n",
      "page_content='{\"page_content\": [\"19 Conclusion\\\\nThis lab covered a lot of ground! We have several key takeaways that you will be expected to apply\\\\nin future labs:\\\\n\\\\u2022How to apply the Bag of Words approach using multi-hot encoding and a dense network\\\\n\\\\u2022What the impact of the batch size is on the training process\\\\n\\\\u2022How to use the built in evaluate() method to test a model14\"], \"metadata\": {\"source\": \"decrypted_SEC595 - Workbook 2_2036060.pdf\", \"page\": 150}}' metadata={'source': 'C:\\\\Users\\\\nc\\\\Desktop\\\\projects\\\\sec595\\\\docs.json', 'seq_num': 2926}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "documents.client.indices.refresh(index=\"sec595\")\n",
    "\n",
    "results = documents.similarity_search(\n",
    "    \"What is Bag of Words?\", k=4, strategy=ElasticsearchStore.SparseVectorRetrievalStrategy(\n",
    "    model_id=\".elser_model_2_linux-x86_64\"\n",
    ")\n",
    ")\n",
    "showResults(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making some adjustments and indexing this way in v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = []\n",
    "content = []\n",
    "\n",
    "for books in processed_data:\n",
    "    for page in books:\n",
    "        metadata.append(page.metadata)\n",
    "        content.append(page.page_content)\n",
    "    \n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=512, chunk_overlap=256\n",
    ")\n",
    "docs = text_splitter.create_documents(content[0], metadatas=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the baseline index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = []\n",
    "for index in indexes:\n",
    "    lines = index.page_content.split('\\n')\n",
    "    records = []\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for line in lines[1:]:\n",
    "        if not re.match(r'^\\d+', line):\n",
    "            records.append(line)\n",
    "            txt = line.split(',')\n",
    "            idx.append({'topic': txt[0], 'book': index.metadata['source'], 'pages': [tx.strip() for tx in txt[1:]]})\n",
    "            topic = txt[0]\n",
    "            j = int(i)\n",
    "            i += 1\n",
    "        else:\n",
    "            if j < len(records):\n",
    "                records[j] = records[j] + ' ' + line\n",
    "                txt = line.split(', ')\n",
    "                idx[j]['pages'] = idx[j]['pages'] + txt[:]\n",
    "                i += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic: Epsilon pages ['seeDBSCAN'] row: Elbow Method\n",
      "topic: latentspace pages ['22'] row: Huberloss\n",
      "topic: Leaky Rectified Linear Unit pages ['52', '53'] row: IMDB dataset\n",
      "topic: linear regression pages ['6'] row: IMDB dataset\n",
      "topic: normal forms pages ['63', '74', '89', '90', '94', '95'] row: MongoEngine\n",
      "topic: NumPy pages ['3', '7'] row: NetFlow\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic</th>\n",
       "      <th>book</th>\n",
       "      <th>pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Anaconda</td>\n",
       "      <td>Book 1</td>\n",
       "      <td>[10, 11, 47, 74, 8]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BackBlaze</td>\n",
       "      <td>Book 2</td>\n",
       "      <td>[28]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>Book 4</td>\n",
       "      <td>[45]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayes Theorem</td>\n",
       "      <td>Book 2</td>\n",
       "      <td>[56–58]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bayesian</td>\n",
       "      <td>Book 4</td>\n",
       "      <td>[45]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           topic    book                pages\n",
       "0       Anaconda  Book 1  [10, 11, 47, 74, 8]\n",
       "1      BackBlaze  Book 2                 [28]\n",
       "2   Bag of Words  Book 4                 [45]\n",
       "3  Bayes Theorem  Book 2              [56–58]\n",
       "4       Bayesian  Book 4                 [45]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# This function is like 99% there. I broke my brain trying to handle the edge cases.\n",
    "# Since there are two columns, it is sometimes difficult to know when to start a new topic.\n",
    "\n",
    "df = pd.DataFrame(idx)\n",
    "df = df.explode('pages')\n",
    "# group by topic and book\n",
    "df = df.groupby(['topic', 'book'])['pages'].apply(list).reset_index()\n",
    "df['book'] = df['book'].apply(lambda x: x.split(' - ')[1].split('_')[0])\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    for j, page in enumerate(row['pages']):\n",
    "        if isinstance(page, str):\n",
    "            if re.search(r'[a-zA-Z]', page):\n",
    "                if re.search(r'[0-9]', page):\n",
    "                    # Get topic for new row\n",
    "                    newtopic = re.match(r\"\\d{1,3}(.*)\", page).group(1)\n",
    "                    # Adjust value of current page to only contain the digits\n",
    "                    row['pages'][j] = re.match(r\"(\\d{1,3}).*\", page).group(0)\n",
    "                    newpages = row['pages'][j+1:]\n",
    "                    row['pages'] = row['pages'][:j-1]\n",
    "                    print(f'topic: {newtopic} pages {str(newpages)} row: {row[\"topic\"]}')\n",
    "                    dfx = pd.DataFrame({'topic': newtopic, 'book': [row['book']], 'pages': [newpages]})\n",
    "                    df = pd.concat([df, dfx], axis=0)\n",
    "                    \n",
    "# sort df by book and  topic\n",
    "df = df.sort_values(by=['book', 'topic']).reset_index(drop=True)\n",
    "# drop rows with null page\n",
    "df = df.dropna(subset=['pages'])\n",
    "# aggregate by topic & book then deduplicate the items in pages list\n",
    "df = df.groupby(['topic', 'book'])['pages'].apply(lambda x: list(set([item for sublist in x for item in sublist]))).reset_index()\n",
    "# sort the values in each row for pages even if its a str\n",
    "df['pages'] = df['pages'].apply(lambda x: sorted(x, key=lambda y: str(y)))\n",
    "# get rid of empty values in the pages list\n",
    "df['pages'] = df['pages'].apply(lambda x: [i for i in x if i])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do Elastic Stuff\n",
    "We should probably slice the pages up a little  bit more, just in case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "def generate_word_cloud(text):\n",
    "    wordcloud = WordCloud(width = 800, height = 400, background_color ='white').generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Example usage\n",
    "for content_text in contents:\n",
    "    generate_word_cloud(' '.join([page.page_content for page in content_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean it better\n",
    "\n",
    "```python\n",
    "import unicodedata\n",
    "\n",
    "def display_comparison(original, cleaned):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12, 6), sharex=True, sharey=True)\n",
    "    ax[0].text(0.5, 0.5, original, ha='center', va='center', fontsize=12, wrap=True)\n",
    "    ax[0].set_title('Original Text')\n",
    "    ax[0].axis('off')\n",
    "    \n",
    "    ax[1].text(0.5, 0.5, cleaned, ha='center', va='center', fontsize=12, wrap=True)\n",
    "    ax[1].set_title('Cleaned Text')\n",
    "    ax[1].axis('off')\n",
    "    plt.rcParams['font.family'] = 'DejaVu Sans' \n",
    "    plt.show()\n",
    "\n",
    "# Example usage with dummy text\n",
    "clean = []\n",
    "for pdf in pdfs:\n",
    "    loader = PyPDFLoader(pdf)\n",
    "    pages = loader.load_and_split()\n",
    "    original = [' '.join([page.page_content for page in pages])]\n",
    "    original = unicodedata.normalize('NFKD', str(original))\n",
    "    cleaned_text = [' '.join([clean_document(page.page_content) for page in pages])]\n",
    "    cleaned_text = unicodedata.normalize('NFKD', str(cleaned_text))\n",
    "\n",
    "    display_comparison(original, cleaned_text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facilitating the review process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation & Parsing of the PDF Files\n",
    "\n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
